(mountainCarContinuous) ali@Mac PG % python pg.py
2025-07-09 19:06:20,906	INFO worker.py:1724 -- Started a local Ray instance.

[INFO] Starting PG training on RandomizedMountainCarContinuous-v1 with RANDOMIZED INITIAL STATES...

2025-07-09 19:06:21,418	WARNING deprecation.py:50 -- DeprecationWarning: `rllib/algorithms/pg/` has been deprecated. Use `rllib_contrib/pg/` instead. This will raise an error in the future!
2025-07-09 19:06:22,246	WARNING util.py:62 -- Install gputil for GPU system monitoring.
2025-07-09 19:06:22,329	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!
Iteration 1: episode_reward_mean = nan
Iteration 2: episode_reward_mean = nan
Iteration 3: episode_reward_mean = nan
Iteration 4: episode_reward_mean = nan
Iteration 5: episode_reward_mean = nan
Iteration 6: episode_reward_mean = nan
Iteration 7: episode_reward_mean = nan
Iteration 8: episode_reward_mean = nan
Iteration 9: episode_reward_mean = nan
Iteration 10: episode_reward_mean = -1.09
Iteration 11: episode_reward_mean = -1.09
Iteration 12: episode_reward_mean = -1.09
Iteration 13: episode_reward_mean = -1.09
Iteration 14: episode_reward_mean = -1.09
Iteration 15: episode_reward_mean = -1.09
Iteration 16: episode_reward_mean = -1.09
Iteration 17: episode_reward_mean = -1.09
Iteration 18: episode_reward_mean = -1.09
Iteration 19: episode_reward_mean = -1.09
Iteration 20: episode_reward_mean = -1.09
Iteration 21: episode_reward_mean = -1.09
Iteration 22: episode_reward_mean = -1.09
Iteration 23: episode_reward_mean = -1.09
Iteration 24: episode_reward_mean = -1.09
Iteration 25: episode_reward_mean = 16.41
Iteration 26: episode_reward_mean = 16.41
Iteration 27: episode_reward_mean = 16.41
Iteration 28: episode_reward_mean = 16.41
Iteration 29: episode_reward_mean = 16.41
Iteration 30: episode_reward_mean = 16.41
Iteration 31: episode_reward_mean = 16.41
Iteration 32: episode_reward_mean = 16.41
Iteration 33: episode_reward_mean = 16.41
Iteration 34: episode_reward_mean = 16.41
Iteration 35: episode_reward_mean = 16.41
Iteration 36: episode_reward_mean = 16.41
Iteration 37: episode_reward_mean = 16.41
Iteration 38: episode_reward_mean = 5.17
Iteration 39: episode_reward_mean = 5.17
Iteration 40: episode_reward_mean = 5.17
Iteration 41: episode_reward_mean = 5.17
Iteration 42: episode_reward_mean = 5.17
Iteration 43: episode_reward_mean = 5.17
Iteration 44: episode_reward_mean = 5.17
Iteration 45: episode_reward_mean = 5.17
Iteration 46: episode_reward_mean = 5.17
Iteration 47: episode_reward_mean = 5.17
Iteration 48: episode_reward_mean = 0.98
Iteration 49: episode_reward_mean = 0.98
Iteration 50: episode_reward_mean = 0.98
Iteration 51: episode_reward_mean = 0.98
Iteration 52: episode_reward_mean = 0.98
Iteration 53: episode_reward_mean = 0.98
Iteration 54: episode_reward_mean = 0.98
Iteration 55: episode_reward_mean = 0.98
Iteration 56: episode_reward_mean = 0.98
Iteration 57: episode_reward_mean = 0.98
Iteration 58: episode_reward_mean = 0.98
Iteration 59: episode_reward_mean = 0.98
Iteration 60: episode_reward_mean = 0.98
Iteration 61: episode_reward_mean = 0.98
Iteration 62: episode_reward_mean = 0.98
Iteration 63: episode_reward_mean = 0.98
Iteration 64: episode_reward_mean = 0.98
Iteration 65: episode_reward_mean = -11.69
Iteration 66: episode_reward_mean = -11.69
Iteration 67: episode_reward_mean = -11.69
Iteration 68: episode_reward_mean = -11.69
Iteration 69: episode_reward_mean = -11.69
Iteration 70: episode_reward_mean = -11.69
Iteration 71: episode_reward_mean = -3.70
Iteration 72: episode_reward_mean = -3.70
Iteration 73: episode_reward_mean = -3.70
Iteration 74: episode_reward_mean = -3.70
Iteration 75: episode_reward_mean = -3.70
Iteration 76: episode_reward_mean = -3.70
Iteration 77: episode_reward_mean = -3.70
Iteration 78: episode_reward_mean = -3.70
Iteration 79: episode_reward_mean = -3.70
Iteration 80: episode_reward_mean = -3.70
Iteration 81: episode_reward_mean = -3.70
Iteration 82: episode_reward_mean = -3.70
Iteration 83: episode_reward_mean = -3.70
Iteration 84: episode_reward_mean = -3.70
Iteration 85: episode_reward_mean = -3.70
Iteration 86: episode_reward_mean = -3.70
Iteration 87: episode_reward_mean = -3.70
Iteration 88: episode_reward_mean = -3.70
Iteration 89: episode_reward_mean = -3.70
Iteration 90: episode_reward_mean = -3.70
Iteration 91: episode_reward_mean = -3.70
Iteration 92: episode_reward_mean = -3.70
Iteration 93: episode_reward_mean = -3.70
Iteration 94: episode_reward_mean = -3.70
Iteration 95: episode_reward_mean = -10.74
Iteration 96: episode_reward_mean = -0.53
Iteration 97: episode_reward_mean = -0.53
Iteration 98: episode_reward_mean = -0.53
Iteration 99: episode_reward_mean = -0.53
Iteration 100: episode_reward_mean = -0.53
Iteration 101: episode_reward_mean = -0.53
Iteration 102: episode_reward_mean = -0.53
Iteration 103: episode_reward_mean = -0.53
Iteration 104: episode_reward_mean = -0.53
Iteration 105: episode_reward_mean = -0.53
Iteration 106: episode_reward_mean = -0.53
Iteration 107: episode_reward_mean = -2.81
Iteration 108: episode_reward_mean = -2.81
Iteration 109: episode_reward_mean = -2.81
Iteration 110: episode_reward_mean = -2.81
Iteration 111: episode_reward_mean = 15.51
Iteration 112: episode_reward_mean = 15.51
Iteration 113: episode_reward_mean = 15.51
Iteration 114: episode_reward_mean = 15.51
Iteration 115: episode_reward_mean = 15.51
Iteration 116: episode_reward_mean = 15.51
Iteration 117: episode_reward_mean = 15.51
Iteration 118: episode_reward_mean = 15.51
Iteration 119: episode_reward_mean = 15.51
Iteration 120: episode_reward_mean = 24.57
Iteration 121: episode_reward_mean = 24.57
Iteration 122: episode_reward_mean = 24.57
Iteration 123: episode_reward_mean = 24.57
Iteration 124: episode_reward_mean = 24.57
Iteration 125: episode_reward_mean = 24.57
Iteration 126: episode_reward_mean = 24.57
Iteration 127: episode_reward_mean = 24.57
Iteration 128: episode_reward_mean = 24.57
Iteration 129: episode_reward_mean = 24.57
Iteration 130: episode_reward_mean = 26.39
Iteration 131: episode_reward_mean = 26.39
Iteration 132: episode_reward_mean = 26.39
Iteration 133: episode_reward_mean = 26.39
Iteration 134: episode_reward_mean = 26.39
Iteration 135: episode_reward_mean = 26.39
Iteration 136: episode_reward_mean = 26.39
Iteration 137: episode_reward_mean = 26.39
Iteration 138: episode_reward_mean = 26.39
Iteration 139: episode_reward_mean = 26.39
Iteration 140: episode_reward_mean = 26.39
Iteration 141: episode_reward_mean = 26.39
Iteration 142: episode_reward_mean = 26.39
Iteration 143: episode_reward_mean = 26.39
Iteration 144: episode_reward_mean = 22.77
Iteration 145: episode_reward_mean = 22.77
Iteration 146: episode_reward_mean = 22.77
Iteration 147: episode_reward_mean = 25.02
Iteration 148: episode_reward_mean = 25.02
Iteration 149: episode_reward_mean = 25.02
Iteration 150: episode_reward_mean = 25.02
Iteration 151: episode_reward_mean = 25.02
Iteration 152: episode_reward_mean = 25.02
Iteration 153: episode_reward_mean = 25.02
Iteration 154: episode_reward_mean = 25.02
Iteration 155: episode_reward_mean = 25.02
Iteration 156: episode_reward_mean = 25.02
Iteration 157: episode_reward_mean = 23.79
Iteration 158: episode_reward_mean = 30.12
Iteration 159: episode_reward_mean = 30.12
Iteration 160: episode_reward_mean = 30.12
Iteration 161: episode_reward_mean = 30.12
Iteration 162: episode_reward_mean = 30.12
Iteration 163: episode_reward_mean = 30.12
Iteration 164: episode_reward_mean = 30.12
Iteration 165: episode_reward_mean = 30.12
Iteration 166: episode_reward_mean = 30.12
Iteration 167: episode_reward_mean = 30.12
Iteration 168: episode_reward_mean = 30.12
Iteration 169: episode_reward_mean = 30.12
Iteration 170: episode_reward_mean = 30.12
Iteration 171: episode_reward_mean = 30.12
Iteration 172: episode_reward_mean = 30.12
Iteration 173: episode_reward_mean = 30.12
Iteration 174: episode_reward_mean = 30.12
Iteration 175: episode_reward_mean = 30.12
Iteration 176: episode_reward_mean = 30.12
Iteration 177: episode_reward_mean = 30.12
Iteration 178: episode_reward_mean = 30.12
Iteration 179: episode_reward_mean = 30.12
Iteration 180: episode_reward_mean = 27.76
Iteration 181: episode_reward_mean = 27.76
Iteration 182: episode_reward_mean = 27.76
Iteration 183: episode_reward_mean = 27.76
Iteration 184: episode_reward_mean = 27.76
Iteration 185: episode_reward_mean = 27.76
Iteration 186: episode_reward_mean = 27.76
Iteration 187: episode_reward_mean = 27.76
Iteration 188: episode_reward_mean = 27.76
Iteration 189: episode_reward_mean = 27.76
Iteration 190: episode_reward_mean = 27.76
Iteration 191: episode_reward_mean = 27.76
Iteration 192: episode_reward_mean = 27.76
Iteration 193: episode_reward_mean = 27.76
Iteration 194: episode_reward_mean = 27.76
Iteration 195: episode_reward_mean = 27.76
Iteration 196: episode_reward_mean = 27.76
Iteration 197: episode_reward_mean = 27.76
Iteration 198: episode_reward_mean = 27.76
Iteration 199: episode_reward_mean = 24.32
Iteration 200: episode_reward_mean = 24.32
Iteration 201: episode_reward_mean = 24.32
Iteration 202: episode_reward_mean = 24.32
Iteration 203: episode_reward_mean = 33.13
Iteration 204: episode_reward_mean = 35.01
Iteration 205: episode_reward_mean = 35.01
Iteration 206: episode_reward_mean = 35.01
Iteration 207: episode_reward_mean = 39.78
Iteration 208: episode_reward_mean = 41.21
Iteration 209: episode_reward_mean = 41.21
Iteration 210: episode_reward_mean = 41.21
Iteration 211: episode_reward_mean = 41.21
Iteration 212: episode_reward_mean = 41.21
Iteration 213: episode_reward_mean = 41.21
Iteration 214: episode_reward_mean = 41.21
Iteration 215: episode_reward_mean = 41.21
Iteration 216: episode_reward_mean = 41.21
Iteration 217: episode_reward_mean = 41.21
Iteration 218: episode_reward_mean = 41.21
Iteration 219: episode_reward_mean = 41.21
Iteration 220: episode_reward_mean = 41.21
Iteration 221: episode_reward_mean = 41.21
Iteration 222: episode_reward_mean = 41.21
Iteration 223: episode_reward_mean = 41.21
Iteration 224: episode_reward_mean = 41.21
Iteration 225: episode_reward_mean = 41.21
Iteration 226: episode_reward_mean = 41.21
Iteration 227: episode_reward_mean = 41.21
Iteration 228: episode_reward_mean = 41.21
Iteration 229: episode_reward_mean = 41.21
Iteration 230: episode_reward_mean = 41.21
Iteration 231: episode_reward_mean = 41.21
Iteration 232: episode_reward_mean = 41.21
Iteration 233: episode_reward_mean = 41.21
Iteration 234: episode_reward_mean = 41.21
Iteration 235: episode_reward_mean = 41.21
Iteration 236: episode_reward_mean = 41.21
Iteration 237: episode_reward_mean = 41.21
Iteration 238: episode_reward_mean = 41.21
Iteration 239: episode_reward_mean = 41.21
Iteration 240: episode_reward_mean = 41.21
Iteration 241: episode_reward_mean = 41.21
Iteration 242: episode_reward_mean = 41.21
Iteration 243: episode_reward_mean = 41.21
Iteration 244: episode_reward_mean = 41.21
Iteration 245: episode_reward_mean = 41.21
Iteration 246: episode_reward_mean = 41.21
Iteration 247: episode_reward_mean = 41.21
Iteration 248: episode_reward_mean = 41.21
Iteration 249: episode_reward_mean = 41.21
Iteration 250: episode_reward_mean = 41.21
Iteration 251: episode_reward_mean = 41.21
Iteration 252: episode_reward_mean = 41.21
Iteration 253: episode_reward_mean = 41.21
Iteration 254: episode_reward_mean = 41.21
Iteration 255: episode_reward_mean = 41.21
Iteration 256: episode_reward_mean = 41.21
Iteration 257: episode_reward_mean = 41.21
Iteration 258: episode_reward_mean = 41.21
Iteration 259: episode_reward_mean = 41.21
Iteration 260: episode_reward_mean = 41.21
Iteration 261: episode_reward_mean = 41.21
Iteration 262: episode_reward_mean = 41.21
Iteration 263: episode_reward_mean = 41.21
Iteration 264: episode_reward_mean = 41.21
Iteration 265: episode_reward_mean = 41.21
Iteration 266: episode_reward_mean = 41.21
Iteration 267: episode_reward_mean = 41.21
Iteration 268: episode_reward_mean = 41.21
Iteration 269: episode_reward_mean = 41.21
Iteration 270: episode_reward_mean = 41.21
Iteration 271: episode_reward_mean = 41.21
Iteration 272: episode_reward_mean = 41.21
Iteration 273: episode_reward_mean = 41.21
Iteration 274: episode_reward_mean = 41.21
Iteration 275: episode_reward_mean = 41.21
Iteration 276: episode_reward_mean = 41.21
Iteration 277: episode_reward_mean = 41.21
Iteration 278: episode_reward_mean = 41.21
Iteration 279: episode_reward_mean = 41.21
Iteration 280: episode_reward_mean = 41.21
Iteration 281: episode_reward_mean = 41.21
Iteration 282: episode_reward_mean = 41.21
Iteration 283: episode_reward_mean = 41.21
Iteration 284: episode_reward_mean = 41.21
Iteration 285: episode_reward_mean = 41.21
Iteration 286: episode_reward_mean = 41.21
Iteration 287: episode_reward_mean = 41.21
Iteration 288: episode_reward_mean = 41.21
Iteration 289: episode_reward_mean = 41.21
Iteration 290: episode_reward_mean = 41.21
Iteration 291: episode_reward_mean = 41.21
Iteration 292: episode_reward_mean = 41.21
Iteration 293: episode_reward_mean = 41.21
Iteration 294: episode_reward_mean = 41.21
Iteration 295: episode_reward_mean = 41.21
Iteration 296: episode_reward_mean = 41.21
Iteration 297: episode_reward_mean = 41.21
Iteration 298: episode_reward_mean = 41.21
Iteration 299: episode_reward_mean = 41.21
Iteration 300: episode_reward_mean = 16.26
2025-07-09 19:06:47,279	WARNING policy.py:137 -- Can not figure out a durable policy name for <class 'ray.rllib.algorithms.pg.pg_torch_policy.PGTorchPolicy'>. You are probably trying to checkpoint a custom policy. Raw policy class may cause problems when the checkpoint needs to be loaded in the future. To fix this, make sure you add your custom policy in rllib.algorithms.registry.POLICIES.

[INFO] Checkpoint saved at: /var/folders/41/3k1hxlt53hxbc7jr4s6jf_980000gn/T/tmplp7hyta8

[INFO] Training Complete
Avg Training CPU Usage: 22.33%
Peak Memory Usage: 399.23 MB
Final Memory Change: 190.64 MB
Training Time: 26.87 seconds

Running trained PG policy on RandomizedMountainCarContinuous-v1 and logging state evolution...

2025-07-09 19:06:48,315	WARNING util.py:62 -- Install gputil for GPU system monitoring.
2025-07-09 19:06:48,333	INFO trainable.py:585 -- Restored on 127.0.0.1 from checkpoint: Checkpoint(filesystem=local, path=/var/folders/41/3k1hxlt53hxbc7jr4s6jf_980000gn/T/tmplp7hyta8)
Inference log saved to mountaincarcontinuous_inference_PG_log.csv

State plot saved to PG-MountainCarContinuous-v1.png

