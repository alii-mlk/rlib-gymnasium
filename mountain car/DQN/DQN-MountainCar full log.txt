(avis) ali@Mac DQN % python dqn.py 
/Users/ali/miniforge3/envs/avis/lib/python3.10/site-packages/ray/air/_internal/remote_storage.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging
2025-07-08 22:12:15,196	INFO worker.py:1528 -- Started a local Ray instance.

[INFO] Starting DQN training on RandomizedMountainCar-v1 with RANDOMIZED INITIAL STATES...

WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1752005535.838349 2900399 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers
2025-07-08 22:12:15,846	INFO simple_q.py:307 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting `simple_optimizer=True` if this doesn't work for you.
2025-07-08 22:12:15,846	INFO algorithm.py:457 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
2025-07-08 22:12:16,579	WARNING util.py:66 -- Install gputil for GPU system monitoring.
2025-07-08 22:12:16,593	WARNING multi_agent_prioritized_replay_buffer.py:215 -- Adding batches with column `weights` to this buffer while providing weights as a call argument to the add method results in the column being overwritten.
Iteration 1: episode_reward_mean = -139.83
2025-07-08 22:12:17,158	WARNING deprecation.py:47 -- DeprecationWarning: `concat_samples` has been deprecated. Use `concat_samples() from rllib.policy.sample_batch` instead. This will raise an error in the future!
Iteration 2: episode_reward_mean = -167.18
Iteration 3: episode_reward_mean = -160.00
Iteration 4: episode_reward_mean = -168.70
Iteration 5: episode_reward_mean = -164.17
Iteration 6: episode_reward_mean = -148.95
Iteration 7: episode_reward_mean = -144.25
Iteration 8: episode_reward_mean = -145.81
Iteration 9: episode_reward_mean = -147.25
Iteration 10: episode_reward_mean = -146.46
Iteration 11: episode_reward_mean = -141.64
Iteration 12: episode_reward_mean = -142.42
Iteration 13: episode_reward_mean = -143.79
Iteration 14: episode_reward_mean = -141.48
Iteration 15: episode_reward_mean = -130.39
Iteration 16: episode_reward_mean = -120.70
Iteration 17: episode_reward_mean = -114.13
Iteration 18: episode_reward_mean = -111.75
Iteration 19: episode_reward_mean = -106.72
Iteration 20: episode_reward_mean = -104.38
Iteration 21: episode_reward_mean = -94.41
Iteration 22: episode_reward_mean = -82.62
Iteration 23: episode_reward_mean = -83.54
Iteration 24: episode_reward_mean = -78.48
Iteration 25: episode_reward_mean = -72.39
Iteration 26: episode_reward_mean = -67.16

[INFO] Early stopping at iteration 26 — reward stabilized ≥ -90 for 5 iterations.


[INFO] Checkpoint saved at: /Users/ali/ray_results/DQN_RandomizedMountainCar-v1_2025-07-08_22-12-15k4jqkzn4/checkpoint_000026

[INFO] Training Complete
Avg Training CPU Usage: 17.16%
Peak Memory Usage: 521.77 MB
Final Memory Change: 258.52 MB
Training Time: 53.04 seconds

Running trained DQN policy on RandomizedMountainCar-v1 and logging state evolution...

2025-07-08 22:13:08,875	WARNING util.py:66 -- Install gputil for GPU system monitoring.
2025-07-08 22:13:08,882	INFO trainable.py:766 -- Restored on 127.0.0.1 from checkpoint: /Users/ali/ray_results/DQN_RandomizedMountainCar-v1_2025-07-08_22-12-15k4jqkzn4/checkpoint_000026
2025-07-08 22:13:08,882	INFO trainable.py:775 -- Current state after restoring: {'_iteration': 26, '_timesteps_total': None, '_time_total': 52.0372211933136, '_episodes_total': 258}
Inference log saved to mountaincar_inference_DQN_log.csv

State plot saved to DQN-MountainCar-v1.png