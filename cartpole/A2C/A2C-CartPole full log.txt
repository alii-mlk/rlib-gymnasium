(avis) ali@Mac A2C % python a2c.py 
/Users/ali/miniforge3/envs/avis/lib/python3.10/site-packages/ray/air/_internal/remote_storage.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging
2025-07-07 15:19:46,241	INFO worker.py:1528 -- Started a local Ray instance.
(raylet) /Users/ali/miniforge3/envs/avis/lib/python3.10/site-packages/ray/air/_internal/remote_storage.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
(raylet)   from pkg_resources import packaging
(raylet) /Users/ali/miniforge3/envs/avis/lib/python3.10/site-packages/ray/air/_internal/remote_storage.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
(raylet)   from pkg_resources import packaging
(raylet) /Users/ali/miniforge3/envs/avis/lib/python3.10/site-packages/ray/air/_internal/remote_storage.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
(raylet)   from pkg_resources import packaging
(raylet) /Users/ali/miniforge3/envs/avis/lib/python3.10/site-packages/ray/air/_internal/remote_storage.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
(raylet)   from pkg_resources import packaging
(raylet) /Users/ali/miniforge3/envs/avis/lib/python3.10/site-packages/ray/air/_internal/remote_storage.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
(raylet)   from pkg_resources import packaging

Starting A2C training on RandomizedCartPole-v1 with RANDOMIZED INITIAL STATES...

WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1751894386.873410 2023803 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers
2025-07-07 15:19:46,879	WARNING a2c.py:144 -- `train_batch_size` (32) cannot be smaller than sample_batch_size (`rollout_fragment_length` x `num_workers` x `num_envs_per_worker`) (40) when micro-batching is not set. This is to ensure that only on gradient update is applied to policy in every iteration on the entire collected batch. As a result of we do not change the policy too much before we sample again and stay on policy as much as possible. This will help the learning stability. Setting train_batch_size = 40.
2025-07-07 15:19:46,880	INFO algorithm.py:457 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
(raylet) /Users/ali/miniforge3/envs/avis/lib/python3.10/site-packages/ray/air/_internal/remote_storage.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
(raylet)   from pkg_resources import packaging
(raylet) /Users/ali/miniforge3/envs/avis/lib/python3.10/site-packages/ray/air/_internal/remote_storage.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
(raylet)   from pkg_resources import packaging
2025-07-07 15:19:49,859	WARNING util.py:66 -- Install gputil for GPU system monitoring.
Iteration 1: episode_reward_mean = 35.36
Iteration 2: episode_reward_mean = 79.44
Iteration 3: episode_reward_mean = 90.42
Iteration 4: episode_reward_mean = 96.73
Iteration 5: episode_reward_mean = 100.48
Iteration 6: episode_reward_mean = 104.99
Iteration 7: episode_reward_mean = 122.73
Iteration 8: episode_reward_mean = 132.81
Iteration 9: episode_reward_mean = 178.64
Iteration 10: episode_reward_mean = 241.47
Iteration 11: episode_reward_mean = 306.35
Iteration 12: episode_reward_mean = 327.19
Iteration 13: episode_reward_mean = 354.65
Iteration 14: episode_reward_mean = 393.89
Iteration 15: episode_reward_mean = 404.37
Iteration 16: episode_reward_mean = 427.56
Iteration 17: episode_reward_mean = 426.10
Iteration 18: episode_reward_mean = 426.61
Iteration 19: episode_reward_mean = 437.80
Iteration 20: episode_reward_mean = 435.26
Iteration 21: episode_reward_mean = 406.57
Iteration 22: episode_reward_mean = 436.47
Iteration 23: episode_reward_mean = 439.24
Iteration 24: episode_reward_mean = 425.83
Iteration 25: episode_reward_mean = 450.51
Iteration 26: episode_reward_mean = 402.39
Iteration 27: episode_reward_mean = 423.37
Iteration 28: episode_reward_mean = 396.79
Iteration 29: episode_reward_mean = 441.39
Iteration 30: episode_reward_mean = 422.42
Iteration 31: episode_reward_mean = 453.42
Iteration 32: episode_reward_mean = 420.76
Iteration 33: episode_reward_mean = 412.29
Iteration 34: episode_reward_mean = 433.58
Iteration 35: episode_reward_mean = 435.43
Iteration 36: episode_reward_mean = 421.82
Iteration 37: episode_reward_mean = 431.22
Iteration 38: episode_reward_mean = 415.18
Iteration 39: episode_reward_mean = 424.05
Iteration 40: episode_reward_mean = 428.09
Iteration 41: episode_reward_mean = 430.19
Iteration 42: episode_reward_mean = 436.74
Iteration 43: episode_reward_mean = 436.74
Iteration 44: episode_reward_mean = 407.80
Iteration 45: episode_reward_mean = 431.10
Iteration 46: episode_reward_mean = 440.82
Iteration 47: episode_reward_mean = 426.31
Iteration 48: episode_reward_mean = 416.59
Iteration 49: episode_reward_mean = 390.50
Iteration 50: episode_reward_mean = 422.93
Iteration 51: episode_reward_mean = 389.02
Iteration 52: episode_reward_mean = 427.61
Iteration 53: episode_reward_mean = 452.72
Iteration 54: episode_reward_mean = 442.06
Iteration 55: episode_reward_mean = 425.87
Iteration 56: episode_reward_mean = 418.25
Iteration 57: episode_reward_mean = 419.98
Iteration 58: episode_reward_mean = 412.29
Iteration 59: episode_reward_mean = 422.69
Iteration 60: episode_reward_mean = 455.23
Iteration 61: episode_reward_mean = 450.59
Iteration 62: episode_reward_mean = 423.66
Iteration 63: episode_reward_mean = 422.00
Iteration 64: episode_reward_mean = 411.16
Iteration 65: episode_reward_mean = 446.99
Iteration 66: episode_reward_mean = 411.90
Iteration 67: episode_reward_mean = 415.83
Iteration 68: episode_reward_mean = 399.29
Iteration 69: episode_reward_mean = 416.17
Iteration 70: episode_reward_mean = 450.60
Iteration 71: episode_reward_mean = 431.83
Iteration 72: episode_reward_mean = 379.43
Iteration 73: episode_reward_mean = 436.43
Iteration 74: episode_reward_mean = 416.53
Iteration 75: episode_reward_mean = 446.96
Iteration 76: episode_reward_mean = 431.38
Iteration 77: episode_reward_mean = 431.97
Iteration 78: episode_reward_mean = 417.10
Iteration 79: episode_reward_mean = 436.98
Iteration 80: episode_reward_mean = 429.67
Iteration 81: episode_reward_mean = 428.67
Iteration 82: episode_reward_mean = 400.64
Iteration 83: episode_reward_mean = 424.59
Iteration 84: episode_reward_mean = 430.84
Iteration 85: episode_reward_mean = 435.38
Iteration 86: episode_reward_mean = 387.38
Iteration 87: episode_reward_mean = 404.12
Iteration 88: episode_reward_mean = 441.32
Iteration 89: episode_reward_mean = 442.16
Iteration 90: episode_reward_mean = 434.68
Iteration 91: episode_reward_mean = 402.86
Iteration 92: episode_reward_mean = 431.44
Iteration 93: episode_reward_mean = 433.35
Iteration 94: episode_reward_mean = 444.34
Iteration 95: episode_reward_mean = 434.71
Iteration 96: episode_reward_mean = 450.68
Iteration 97: episode_reward_mean = 432.10
Iteration 98: episode_reward_mean = 425.83
Iteration 99: episode_reward_mean = 424.63
Iteration 100: episode_reward_mean = 426.66
Iteration 101: episode_reward_mean = 423.49
Iteration 102: episode_reward_mean = 394.87
Iteration 103: episode_reward_mean = 411.06
Iteration 104: episode_reward_mean = 445.06
Iteration 105: episode_reward_mean = 406.67
Iteration 106: episode_reward_mean = 421.05
Iteration 107: episode_reward_mean = 413.38
Iteration 108: episode_reward_mean = 403.03
Iteration 109: episode_reward_mean = 405.88
Iteration 110: episode_reward_mean = 431.98
Iteration 111: episode_reward_mean = 433.04
Iteration 112: episode_reward_mean = 409.32
Iteration 113: episode_reward_mean = 417.54
Iteration 114: episode_reward_mean = 456.17
Iteration 115: episode_reward_mean = 404.27
Iteration 116: episode_reward_mean = 420.80
Iteration 117: episode_reward_mean = 424.32
Iteration 118: episode_reward_mean = 426.85
Iteration 119: episode_reward_mean = 427.46
Iteration 120: episode_reward_mean = 443.74
Iteration 121: episode_reward_mean = 438.72
Iteration 122: episode_reward_mean = 425.50
Iteration 123: episode_reward_mean = 390.83
Iteration 124: episode_reward_mean = 437.95
Iteration 125: episode_reward_mean = 420.52
Iteration 126: episode_reward_mean = 429.23
Iteration 127: episode_reward_mean = 444.25
Iteration 128: episode_reward_mean = 445.25
Iteration 129: episode_reward_mean = 447.86
Iteration 130: episode_reward_mean = 437.94
Iteration 131: episode_reward_mean = 442.67
Iteration 132: episode_reward_mean = 413.86
Iteration 133: episode_reward_mean = 431.97
Iteration 134: episode_reward_mean = 438.32
Iteration 135: episode_reward_mean = 381.55
Iteration 136: episode_reward_mean = 419.96
Iteration 137: episode_reward_mean = 416.83
Iteration 138: episode_reward_mean = 425.89
Iteration 139: episode_reward_mean = 421.53
Iteration 140: episode_reward_mean = 430.29
Iteration 141: episode_reward_mean = 426.41
Iteration 142: episode_reward_mean = 441.92
Iteration 143: episode_reward_mean = 444.37
Iteration 144: episode_reward_mean = 437.35
Iteration 145: episode_reward_mean = 432.90
Iteration 146: episode_reward_mean = 431.34
Iteration 147: episode_reward_mean = 397.25
Iteration 148: episode_reward_mean = 428.39
Iteration 149: episode_reward_mean = 424.30
Iteration 150: episode_reward_mean = 425.30
Iteration 151: episode_reward_mean = 437.77
Iteration 152: episode_reward_mean = 440.63
Iteration 153: episode_reward_mean = 439.68
Iteration 154: episode_reward_mean = 424.52
Iteration 155: episode_reward_mean = 428.42
Iteration 156: episode_reward_mean = 426.16
Iteration 157: episode_reward_mean = 454.62
Iteration 158: episode_reward_mean = 425.91
Iteration 159: episode_reward_mean = 429.58
Iteration 160: episode_reward_mean = 432.60
Iteration 161: episode_reward_mean = 431.24
Iteration 162: episode_reward_mean = 420.95
Iteration 163: episode_reward_mean = 432.22
Iteration 164: episode_reward_mean = 427.55
Iteration 165: episode_reward_mean = 431.82
Iteration 166: episode_reward_mean = 444.74
Iteration 167: episode_reward_mean = 431.82
Iteration 168: episode_reward_mean = 434.78
Iteration 169: episode_reward_mean = 444.49
Iteration 170: episode_reward_mean = 437.53
Iteration 171: episode_reward_mean = 431.33
Iteration 172: episode_reward_mean = 444.86
Iteration 173: episode_reward_mean = 439.76
Iteration 174: episode_reward_mean = 432.62
Iteration 175: episode_reward_mean = 418.04
Iteration 176: episode_reward_mean = 436.92
Iteration 177: episode_reward_mean = 436.29
Iteration 178: episode_reward_mean = 435.07
Iteration 179: episode_reward_mean = 424.10
Iteration 180: episode_reward_mean = 420.92
Iteration 181: episode_reward_mean = 442.83
Iteration 182: episode_reward_mean = 429.81
Iteration 183: episode_reward_mean = 427.26
Iteration 184: episode_reward_mean = 411.47
Iteration 185: episode_reward_mean = 414.21
Iteration 186: episode_reward_mean = 426.37
Iteration 187: episode_reward_mean = 421.65
Iteration 188: episode_reward_mean = 414.09
Iteration 189: episode_reward_mean = 433.19
Iteration 190: episode_reward_mean = 444.40
Iteration 191: episode_reward_mean = 444.85
Iteration 192: episode_reward_mean = 439.96
Iteration 193: episode_reward_mean = 451.23
Iteration 194: episode_reward_mean = 444.92
Iteration 195: episode_reward_mean = 413.09
Iteration 196: episode_reward_mean = 429.74
Iteration 197: episode_reward_mean = 401.79
Iteration 198: episode_reward_mean = 438.25
Iteration 199: episode_reward_mean = 444.15
Iteration 200: episode_reward_mean = 429.83
Iteration 201: episode_reward_mean = 440.48
Iteration 202: episode_reward_mean = 454.80
Iteration 203: episode_reward_mean = 432.84
Iteration 204: episode_reward_mean = 438.36
Iteration 205: episode_reward_mean = 444.50
Iteration 206: episode_reward_mean = 435.63
Iteration 207: episode_reward_mean = 450.64
Iteration 208: episode_reward_mean = 430.88
Iteration 209: episode_reward_mean = 436.16
Iteration 210: episode_reward_mean = 440.05
Iteration 211: episode_reward_mean = 438.00
Iteration 212: episode_reward_mean = 438.97
Iteration 213: episode_reward_mean = 429.32
Iteration 214: episode_reward_mean = 420.90
Iteration 215: episode_reward_mean = 435.24
Iteration 216: episode_reward_mean = 445.03
Iteration 217: episode_reward_mean = 441.74
Iteration 218: episode_reward_mean = 436.13
Iteration 219: episode_reward_mean = 442.07
Iteration 220: episode_reward_mean = 453.00
Iteration 221: episode_reward_mean = 448.19
Iteration 222: episode_reward_mean = 436.77
Iteration 223: episode_reward_mean = 422.46
Iteration 224: episode_reward_mean = 429.05
Iteration 225: episode_reward_mean = 429.99
Iteration 226: episode_reward_mean = 447.23
Iteration 227: episode_reward_mean = 448.29
Iteration 228: episode_reward_mean = 433.36
Iteration 229: episode_reward_mean = 426.23
Iteration 230: episode_reward_mean = 445.39
Iteration 231: episode_reward_mean = 446.84
Iteration 232: episode_reward_mean = 438.30
Iteration 233: episode_reward_mean = 444.70
Iteration 234: episode_reward_mean = 446.24
Iteration 235: episode_reward_mean = 468.03
Iteration 236: episode_reward_mean = 434.45
Iteration 237: episode_reward_mean = 429.84
Iteration 238: episode_reward_mean = 448.79
Iteration 239: episode_reward_mean = 436.21
Iteration 240: episode_reward_mean = 440.69
Iteration 241: episode_reward_mean = 445.73
Iteration 242: episode_reward_mean = 444.19
Iteration 243: episode_reward_mean = 450.16
Iteration 244: episode_reward_mean = 443.17
Iteration 245: episode_reward_mean = 446.63
Iteration 246: episode_reward_mean = 462.04
Iteration 247: episode_reward_mean = 448.31
Iteration 248: episode_reward_mean = 426.15
Iteration 249: episode_reward_mean = 427.79
Iteration 250: episode_reward_mean = 432.55
Iteration 251: episode_reward_mean = 403.91
Iteration 252: episode_reward_mean = 415.59
Iteration 253: episode_reward_mean = 456.50
Iteration 254: episode_reward_mean = 446.11
Iteration 255: episode_reward_mean = 449.48
Iteration 256: episode_reward_mean = 448.24
Iteration 257: episode_reward_mean = 457.99
Iteration 258: episode_reward_mean = 454.09
Iteration 259: episode_reward_mean = 443.18
Iteration 260: episode_reward_mean = 451.05
Iteration 261: episode_reward_mean = 435.29
Iteration 262: episode_reward_mean = 421.52
Iteration 263: episode_reward_mean = 431.19
Iteration 264: episode_reward_mean = 430.92
Iteration 265: episode_reward_mean = 434.66
Iteration 266: episode_reward_mean = 442.22
Iteration 267: episode_reward_mean = 423.26
Iteration 268: episode_reward_mean = 423.73
Iteration 269: episode_reward_mean = 417.65
Iteration 270: episode_reward_mean = 438.39
Iteration 271: episode_reward_mean = 448.90
Iteration 272: episode_reward_mean = 434.31
Iteration 273: episode_reward_mean = 442.76
Iteration 274: episode_reward_mean = 441.21
Iteration 275: episode_reward_mean = 432.46
Iteration 276: episode_reward_mean = 419.26
Iteration 277: episode_reward_mean = 419.22
Iteration 278: episode_reward_mean = 429.31
Iteration 279: episode_reward_mean = 439.11
Iteration 280: episode_reward_mean = 449.85
Iteration 281: episode_reward_mean = 450.16
Iteration 282: episode_reward_mean = 439.47
Iteration 283: episode_reward_mean = 436.82
Iteration 284: episode_reward_mean = 443.48
Iteration 285: episode_reward_mean = 432.34
Iteration 286: episode_reward_mean = 436.90
Iteration 287: episode_reward_mean = 445.36
Iteration 288: episode_reward_mean = 431.18
Iteration 289: episode_reward_mean = 433.49
Iteration 290: episode_reward_mean = 421.00
Iteration 291: episode_reward_mean = 418.72
Iteration 292: episode_reward_mean = 426.19
Iteration 293: episode_reward_mean = 432.69
Iteration 294: episode_reward_mean = 438.12
Iteration 295: episode_reward_mean = 423.19
Iteration 296: episode_reward_mean = 435.11
Iteration 297: episode_reward_mean = 429.88
Iteration 298: episode_reward_mean = 437.62
Iteration 299: episode_reward_mean = 446.83
Iteration 300: episode_reward_mean = 431.20

Checkpoint saved at: /Users/ali/ray_results/A2C_RandomizedCartPole-v1_2025-07-07_15-19-46_zvqfysi/checkpoint_000300

Training Complete
Avg Training CPU Usage: 31.59%
Peak Memory Usage: 455.55 MB
Final Memory Change: 60.48 MB
Training Time: 3008.67 seconds

Running trained A2C policy on RandomizedCartPole-v1 and logging state evolution...

2025-07-07 16:09:55,539	WARNING a2c.py:144 -- `train_batch_size` (32) cannot be smaller than sample_batch_size (`rollout_fragment_length` x `num_workers` x `num_envs_per_worker`) (40) when micro-batching is not set. This is to ensure that only on gradient update is applied to policy in every iteration on the entire collected batch. As a result of we do not change the policy too much before we sample again and stay on policy as much as possible. This will help the learning stability. Setting train_batch_size = 40.
(raylet) /Users/ali/miniforge3/envs/avis/lib/python3.10/site-packages/ray/air/_internal/remote_storage.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
(raylet)   from pkg_resources import packaging
(raylet) /Users/ali/miniforge3/envs/avis/lib/python3.10/site-packages/ray/air/_internal/remote_storage.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
(raylet)   from pkg_resources import packaging
2025-07-07 16:10:00,880	WARNING util.py:66 -- Install gputil for GPU system monitoring.
2025-07-07 16:10:00,888	INFO trainable.py:766 -- Restored on 127.0.0.1 from checkpoint: /Users/ali/ray_results/A2C_RandomizedCartPole-v1_2025-07-07_15-19-46_zvqfysi/checkpoint_000300
2025-07-07 16:10:00,888	INFO trainable.py:775 -- Current state after restoring: {'_iteration': 300, '_timesteps_total': None, '_time_total': 3002.911804676056, '_episodes_total': 23243}
Inference log saved to cartpole_inference_A2C_log.csv


