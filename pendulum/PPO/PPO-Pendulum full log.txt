(avis) ali@Mac PPO % python ppo.py
/Users/ali/miniforge3/envs/avis/lib/python3.10/site-packages/ray/air/_internal/remote_storage.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging
2025-07-08 14:26:59,843	INFO worker.py:1528 -- Started a local Ray instance.

[INFO] Starting PPO training on RandomizedPendulum-v1 with RANDOMIZED INITIAL STATES...

WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1751977620.635746 2547306 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers
(raylet) /Users/ali/miniforge3/envs/avis/lib/python3.10/site-packages/ray/air/_internal/remote_storage.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
(raylet)   from pkg_resources import packaging
2025-07-08 14:27:00,642	INFO ppo.py:379 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.
2025-07-08 14:27:00,643	INFO algorithm.py:457 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
(raylet) /Users/ali/miniforge3/envs/avis/lib/python3.10/site-packages/ray/air/_internal/remote_storage.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
(raylet)   from pkg_resources import packaging
(raylet) /Users/ali/miniforge3/envs/avis/lib/python3.10/site-packages/ray/air/_internal/remote_storage.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
(raylet)   from pkg_resources import packaging
2025-07-08 14:27:04,217	WARNING util.py:66 -- Install gputil for GPU system monitoring.
Iteration 1: episode_reward_mean = -1292.07
Iteration 2: episode_reward_mean = -1175.54
Iteration 3: episode_reward_mean = -1142.26
Iteration 4: episode_reward_mean = -1184.76
Iteration 5: episode_reward_mean = -1168.86
Iteration 6: episode_reward_mean = -1155.63
Iteration 7: episode_reward_mean = -1187.11
Iteration 8: episode_reward_mean = -1192.33
Iteration 9: episode_reward_mean = -1143.70
Iteration 10: episode_reward_mean = -1152.52
Iteration 11: episode_reward_mean = -1115.98
Iteration 12: episode_reward_mean = -1091.36
Iteration 13: episode_reward_mean = -1101.30
Iteration 14: episode_reward_mean = -1119.13
Iteration 15: episode_reward_mean = -1099.46
Iteration 16: episode_reward_mean = -1105.39
Iteration 17: episode_reward_mean = -1099.71
Iteration 18: episode_reward_mean = -1062.70
Iteration 19: episode_reward_mean = -1066.17
Iteration 20: episode_reward_mean = -1060.92
Iteration 21: episode_reward_mean = -1055.83
Iteration 22: episode_reward_mean = -1061.01
Iteration 23: episode_reward_mean = -1071.50
Iteration 24: episode_reward_mean = -1027.65
Iteration 25: episode_reward_mean = -1038.66
Iteration 26: episode_reward_mean = -1029.75
Iteration 27: episode_reward_mean = -1009.16
Iteration 28: episode_reward_mean = -1010.46
Iteration 29: episode_reward_mean = -1025.88
Iteration 30: episode_reward_mean = -1016.54
Iteration 31: episode_reward_mean = -1013.78
Iteration 32: episode_reward_mean = -1007.01
Iteration 33: episode_reward_mean = -995.79
Iteration 34: episode_reward_mean = -995.39
Iteration 35: episode_reward_mean = -977.01
Iteration 36: episode_reward_mean = -977.95
Iteration 37: episode_reward_mean = -980.09
Iteration 38: episode_reward_mean = -990.56
Iteration 39: episode_reward_mean = -976.59
Iteration 40: episode_reward_mean = -1010.92
Iteration 41: episode_reward_mean = -1030.68
Iteration 42: episode_reward_mean = -1073.69
Iteration 43: episode_reward_mean = -1068.76
Iteration 44: episode_reward_mean = -1086.48
Iteration 45: episode_reward_mean = -1095.05
Iteration 46: episode_reward_mean = -1094.19
Iteration 47: episode_reward_mean = -1090.25
Iteration 48: episode_reward_mean = -1136.22
Iteration 49: episode_reward_mean = -1129.61
Iteration 50: episode_reward_mean = -1121.59
Iteration 51: episode_reward_mean = -1110.96
Iteration 52: episode_reward_mean = -1094.76
Iteration 53: episode_reward_mean = -1067.03
Iteration 54: episode_reward_mean = -1099.88
Iteration 55: episode_reward_mean = -1091.62
Iteration 56: episode_reward_mean = -1086.20
Iteration 57: episode_reward_mean = -1065.71
Iteration 58: episode_reward_mean = -1056.41
Iteration 59: episode_reward_mean = -1025.99
Iteration 60: episode_reward_mean = -989.00
Iteration 61: episode_reward_mean = -986.54
Iteration 62: episode_reward_mean = -1016.48
Iteration 63: episode_reward_mean = -1025.98
Iteration 64: episode_reward_mean = -1011.68
Iteration 65: episode_reward_mean = -1042.55
Iteration 66: episode_reward_mean = -1047.46
Iteration 67: episode_reward_mean = -1023.66
Iteration 68: episode_reward_mean = -1026.75
Iteration 69: episode_reward_mean = -1042.57
Iteration 70: episode_reward_mean = -1072.75
Iteration 71: episode_reward_mean = -1100.64
Iteration 72: episode_reward_mean = -1113.54
Iteration 73: episode_reward_mean = -1118.43
Iteration 74: episode_reward_mean = -1118.65
Iteration 75: episode_reward_mean = -1101.60
Iteration 76: episode_reward_mean = -1103.53
Iteration 77: episode_reward_mean = -1084.84
Iteration 78: episode_reward_mean = -1081.64
Iteration 79: episode_reward_mean = -1067.61
Iteration 80: episode_reward_mean = -1064.07
Iteration 81: episode_reward_mean = -1047.88
Iteration 82: episode_reward_mean = -1083.90
Iteration 83: episode_reward_mean = -1063.85
Iteration 84: episode_reward_mean = -1079.24
Iteration 85: episode_reward_mean = -1050.92
Iteration 86: episode_reward_mean = -1033.88
Iteration 87: episode_reward_mean = -1026.79
Iteration 88: episode_reward_mean = -1056.14
Iteration 89: episode_reward_mean = -1026.16
Iteration 90: episode_reward_mean = -1035.21
Iteration 91: episode_reward_mean = -1030.75
Iteration 92: episode_reward_mean = -1015.90
Iteration 93: episode_reward_mean = -996.11
Iteration 94: episode_reward_mean = -1026.59
Iteration 95: episode_reward_mean = -999.43
Iteration 96: episode_reward_mean = -1034.29
Iteration 97: episode_reward_mean = -1027.26
Iteration 98: episode_reward_mean = -1018.53
Iteration 99: episode_reward_mean = -1002.23
Iteration 100: episode_reward_mean = -1037.24
Iteration 101: episode_reward_mean = -1014.19
Iteration 102: episode_reward_mean = -1005.08
Iteration 103: episode_reward_mean = -1004.88
Iteration 104: episode_reward_mean = -1030.84
Iteration 105: episode_reward_mean = -1054.36
Iteration 106: episode_reward_mean = -1057.25
Iteration 107: episode_reward_mean = -1069.83
Iteration 108: episode_reward_mean = -1100.54
Iteration 109: episode_reward_mean = -1098.71
Iteration 110: episode_reward_mean = -1071.82
Iteration 111: episode_reward_mean = -1060.82
Iteration 112: episode_reward_mean = -1082.35
Iteration 113: episode_reward_mean = -1055.45
Iteration 114: episode_reward_mean = -1048.07
Iteration 115: episode_reward_mean = -1042.02
Iteration 116: episode_reward_mean = -1031.81
Iteration 117: episode_reward_mean = -994.93
Iteration 118: episode_reward_mean = -997.73
Iteration 119: episode_reward_mean = -981.73
Iteration 120: episode_reward_mean = -1008.26
Iteration 121: episode_reward_mean = -1044.16
Iteration 122: episode_reward_mean = -1056.27
Iteration 123: episode_reward_mean = -1060.59
Iteration 124: episode_reward_mean = -1070.83
Iteration 125: episode_reward_mean = -1055.56
Iteration 126: episode_reward_mean = -1040.09
Iteration 127: episode_reward_mean = -1064.09
Iteration 128: episode_reward_mean = -1075.59
Iteration 129: episode_reward_mean = -1101.41
Iteration 130: episode_reward_mean = -1099.94
Iteration 131: episode_reward_mean = -1087.89
Iteration 132: episode_reward_mean = -1076.56
Iteration 133: episode_reward_mean = -1100.23
Iteration 134: episode_reward_mean = -1090.01
Iteration 135: episode_reward_mean = -1104.04
Iteration 136: episode_reward_mean = -1160.73
Iteration 137: episode_reward_mean = -1179.29
Iteration 138: episode_reward_mean = -1149.60
Iteration 139: episode_reward_mean = -1125.83
Iteration 140: episode_reward_mean = -1139.89
Iteration 141: episode_reward_mean = -1110.94
Iteration 142: episode_reward_mean = -1110.49
Iteration 143: episode_reward_mean = -1091.84
Iteration 144: episode_reward_mean = -1086.55
Iteration 145: episode_reward_mean = -1066.64
Iteration 146: episode_reward_mean = -1072.77
Iteration 147: episode_reward_mean = -1027.06
Iteration 148: episode_reward_mean = -1015.84
Iteration 149: episode_reward_mean = -1052.02
Iteration 150: episode_reward_mean = -1020.47
Iteration 151: episode_reward_mean = -993.50
Iteration 152: episode_reward_mean = -1032.64
Iteration 153: episode_reward_mean = -1061.98
Iteration 154: episode_reward_mean = -1050.79
Iteration 155: episode_reward_mean = -1086.96
Iteration 156: episode_reward_mean = -1104.05
Iteration 157: episode_reward_mean = -1076.54
Iteration 158: episode_reward_mean = -1088.97
Iteration 159: episode_reward_mean = -1080.35
Iteration 160: episode_reward_mean = -1069.97
Iteration 161: episode_reward_mean = -1067.65
Iteration 162: episode_reward_mean = -1070.48
Iteration 163: episode_reward_mean = -1056.63
Iteration 164: episode_reward_mean = -1046.44
Iteration 165: episode_reward_mean = -1071.61
Iteration 166: episode_reward_mean = -1082.11
Iteration 167: episode_reward_mean = -1064.17
Iteration 168: episode_reward_mean = -1099.43
Iteration 169: episode_reward_mean = -1128.17
Iteration 170: episode_reward_mean = -1089.34
Iteration 171: episode_reward_mean = -1078.35
Iteration 172: episode_reward_mean = -1086.24
Iteration 173: episode_reward_mean = -1023.54
Iteration 174: episode_reward_mean = -997.93
Iteration 175: episode_reward_mean = -989.50
Iteration 176: episode_reward_mean = -946.06
Iteration 177: episode_reward_mean = -957.71
Iteration 178: episode_reward_mean = -1004.51
Iteration 179: episode_reward_mean = -1023.67
Iteration 180: episode_reward_mean = -1069.01
Iteration 181: episode_reward_mean = -1124.91
Iteration 182: episode_reward_mean = -1136.92
Iteration 183: episode_reward_mean = -1100.81
Iteration 184: episode_reward_mean = -1083.40
Iteration 185: episode_reward_mean = -1072.06
Iteration 186: episode_reward_mean = -1038.56
Iteration 187: episode_reward_mean = -1034.57
Iteration 188: episode_reward_mean = -1035.57
Iteration 189: episode_reward_mean = -1051.45
Iteration 190: episode_reward_mean = -1062.80
Iteration 191: episode_reward_mean = -1067.94
Iteration 192: episode_reward_mean = -1097.77
Iteration 193: episode_reward_mean = -1090.91
Iteration 194: episode_reward_mean = -1085.76
Iteration 195: episode_reward_mean = -1058.54
Iteration 196: episode_reward_mean = -1101.91
Iteration 197: episode_reward_mean = -1061.87
Iteration 198: episode_reward_mean = -1071.13
Iteration 199: episode_reward_mean = -1077.65
Iteration 200: episode_reward_mean = -1078.03
Iteration 201: episode_reward_mean = -1034.46
Iteration 202: episode_reward_mean = -1033.91
Iteration 203: episode_reward_mean = -1033.81
Iteration 204: episode_reward_mean = -1043.08
Iteration 205: episode_reward_mean = -1051.69
Iteration 206: episode_reward_mean = -1096.32
Iteration 207: episode_reward_mean = -1107.17
Iteration 208: episode_reward_mean = -1124.21
Iteration 209: episode_reward_mean = -1096.64
Iteration 210: episode_reward_mean = -1092.33
Iteration 211: episode_reward_mean = -1051.70
Iteration 212: episode_reward_mean = -1093.44
Iteration 213: episode_reward_mean = -1096.22
Iteration 214: episode_reward_mean = -1110.71
Iteration 215: episode_reward_mean = -1137.25
Iteration 216: episode_reward_mean = -1147.22
Iteration 217: episode_reward_mean = -1103.53
Iteration 218: episode_reward_mean = -1099.64
Iteration 219: episode_reward_mean = -1084.03
Iteration 220: episode_reward_mean = -1049.69
Iteration 221: episode_reward_mean = -1043.62
Iteration 222: episode_reward_mean = -1048.20
Iteration 223: episode_reward_mean = -999.71
Iteration 224: episode_reward_mean = -1015.84
Iteration 225: episode_reward_mean = -1045.45
Iteration 226: episode_reward_mean = -1038.86
Iteration 227: episode_reward_mean = -1037.70
Iteration 228: episode_reward_mean = -1060.06
Iteration 229: episode_reward_mean = -1050.58
Iteration 230: episode_reward_mean = -1031.67
Iteration 231: episode_reward_mean = -1045.23
Iteration 232: episode_reward_mean = -1074.70
Iteration 233: episode_reward_mean = -1090.19
Iteration 234: episode_reward_mean = -1110.85
Iteration 235: episode_reward_mean = -1112.89
Iteration 236: episode_reward_mean = -1098.39
Iteration 237: episode_reward_mean = -1061.41
Iteration 238: episode_reward_mean = -1059.44
Iteration 239: episode_reward_mean = -1052.74
Iteration 240: episode_reward_mean = -1069.85
Iteration 241: episode_reward_mean = -1099.51
Iteration 242: episode_reward_mean = -1102.67
Iteration 243: episode_reward_mean = -1118.89
Iteration 244: episode_reward_mean = -1092.09
Iteration 245: episode_reward_mean = -1072.62
Iteration 246: episode_reward_mean = -1056.11
Iteration 247: episode_reward_mean = -1074.96
Iteration 248: episode_reward_mean = -1073.80
Iteration 249: episode_reward_mean = -1064.48
Iteration 250: episode_reward_mean = -1046.75
Iteration 251: episode_reward_mean = -1039.68
Iteration 252: episode_reward_mean = -1038.30
Iteration 253: episode_reward_mean = -1031.12
Iteration 254: episode_reward_mean = -1077.73
Iteration 255: episode_reward_mean = -1109.29
Iteration 256: episode_reward_mean = -1131.44
Iteration 257: episode_reward_mean = -1099.66
Iteration 258: episode_reward_mean = -1108.60
Iteration 259: episode_reward_mean = -1108.76
Iteration 260: episode_reward_mean = -1100.48
Iteration 261: episode_reward_mean = -1106.68
Iteration 262: episode_reward_mean = -1127.59
Iteration 263: episode_reward_mean = -1139.14
Iteration 264: episode_reward_mean = -1127.00
Iteration 265: episode_reward_mean = -1146.00
Iteration 266: episode_reward_mean = -1108.17
Iteration 267: episode_reward_mean = -1091.20
Iteration 268: episode_reward_mean = -1076.77
Iteration 269: episode_reward_mean = -1063.99
Iteration 270: episode_reward_mean = -1038.16
Iteration 271: episode_reward_mean = -1053.21
Iteration 272: episode_reward_mean = -1044.09
Iteration 273: episode_reward_mean = -1032.81
Iteration 274: episode_reward_mean = -1015.52
Iteration 275: episode_reward_mean = -1007.45
Iteration 276: episode_reward_mean = -1021.80
Iteration 277: episode_reward_mean = -1060.96
Iteration 278: episode_reward_mean = -1053.70
Iteration 279: episode_reward_mean = -1067.77
Iteration 280: episode_reward_mean = -1105.46
Iteration 281: episode_reward_mean = -1098.78
Iteration 282: episode_reward_mean = -1097.84
Iteration 283: episode_reward_mean = -1083.57
Iteration 284: episode_reward_mean = -1079.61
Iteration 285: episode_reward_mean = -1073.44
Iteration 286: episode_reward_mean = -1053.95
Iteration 287: episode_reward_mean = -1044.55
Iteration 288: episode_reward_mean = -1075.93
Iteration 289: episode_reward_mean = -1073.51
Iteration 290: episode_reward_mean = -1073.70
Iteration 291: episode_reward_mean = -1069.76
Iteration 292: episode_reward_mean = -1061.77
Iteration 293: episode_reward_mean = -1041.88
Iteration 294: episode_reward_mean = -1058.37
Iteration 295: episode_reward_mean = -1029.91
Iteration 296: episode_reward_mean = -1040.21
Iteration 297: episode_reward_mean = -1039.16
Iteration 298: episode_reward_mean = -1063.95
Iteration 299: episode_reward_mean = -1057.58
Iteration 300: episode_reward_mean = -1093.22

[INFO] Checkpoint saved at: /Users/ali/ray_results/PPO_RandomizedPendulum-v1_2025-07-08_14-27-00vz6qraib/checkpoint_000300

[INFO] Training Complete
Avg Training CPU Usage: 24.67%
Peak Memory Usage: 367.58 MB
Final Memory Change: 63.00 MB
Training Time: 889.66 seconds

Running trained PPO policy on RandomizedPendulum-v1 and logging state evolution...

(raylet) /Users/ali/miniforge3/envs/avis/lib/python3.10/site-packages/ray/air/_internal/remote_storage.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
(raylet)   from pkg_resources import packaging
(raylet) /Users/ali/miniforge3/envs/avis/lib/python3.10/site-packages/ray/air/_internal/remote_storage.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
(raylet)   from pkg_resources import packaging
2025-07-08 14:41:54,122	WARNING util.py:66 -- Install gputil for GPU system monitoring.
2025-07-08 14:41:54,129	INFO trainable.py:766 -- Restored on 127.0.0.1 from checkpoint: /Users/ali/ray_results/PPO_RandomizedPendulum-v1_2025-07-08_14-27-00vz6qraib/checkpoint_000300
2025-07-08 14:41:54,129	INFO trainable.py:775 -- Current state after restoring: {'_iteration': 300, '_timesteps_total': None, '_time_total': 883.6058766841888, '_episodes_total': 6000}
Inference log saved to pendulum_inference_PPO_log.csv

State plot saved to PPO-Pendulum-v1.png

