(avis) ali@Mac DQN % python dqn.py 
/Users/ali/miniforge3/envs/avis/lib/python3.10/site-packages/ray/air/_internal/remote_storage.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging
2025-07-08 15:14:24,905	INFO worker.py:1528 -- Started a local Ray instance.

[INFO] Starting DQN training on RandomizedPendulum-v1 with RANDOMIZED INITIAL STATES + DISCRETE ACTIONS...

WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1751980465.595841 2579799 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers
2025-07-08 15:14:25,606	INFO simple_q.py:307 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting `simple_optimizer=True` if this doesn't work for you.
2025-07-08 15:14:25,607	INFO algorithm.py:457 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
2025-07-08 15:14:26,431	WARNING util.py:66 -- Install gputil for GPU system monitoring.
2025-07-08 15:14:26,447	WARNING multi_agent_prioritized_replay_buffer.py:215 -- Adding batches with column `weights` to this buffer while providing weights as a call argument to the add method results in the column being overwritten.
Iteration 1: episode_reward_mean = -1239.28
2025-07-08 15:14:27,019	WARNING deprecation.py:47 -- DeprecationWarning: `concat_samples` has been deprecated. Use `concat_samples() from rllib.policy.sample_batch` instead. This will raise an error in the future!
Iteration 2: episode_reward_mean = -1248.95
Iteration 3: episode_reward_mean = -1326.06
Iteration 4: episode_reward_mean = -1364.31
Iteration 5: episode_reward_mean = -1338.13
Iteration 6: episode_reward_mean = -1256.46
Iteration 7: episode_reward_mean = -1197.31
Iteration 8: episode_reward_mean = -1115.72
Iteration 9: episode_reward_mean = -1028.00
Iteration 10: episode_reward_mean = -952.50
Iteration 11: episode_reward_mean = -886.39
Iteration 12: episode_reward_mean = -828.80
Iteration 13: episode_reward_mean = -779.34
Iteration 14: episode_reward_mean = -745.54
Iteration 15: episode_reward_mean = -704.04
Iteration 16: episode_reward_mean = -665.98
Iteration 17: episode_reward_mean = -635.07
Iteration 18: episode_reward_mean = -624.45
Iteration 19: episode_reward_mean = -600.20
Iteration 20: episode_reward_mean = -575.42
Iteration 21: episode_reward_mean = -522.48
Iteration 22: episode_reward_mean = -467.92
Iteration 23: episode_reward_mean = -399.25
Iteration 24: episode_reward_mean = -331.79
Iteration 25: episode_reward_mean = -277.06
Iteration 26: episode_reward_mean = -243.75
Iteration 27: episode_reward_mean = -206.71
Iteration 28: episode_reward_mean = -186.76
Iteration 29: episode_reward_mean = -179.45
Iteration 30: episode_reward_mean = -178.19
Iteration 31: episode_reward_mean = -177.41
Iteration 32: episode_reward_mean = -173.80
Iteration 33: episode_reward_mean = -174.66
Iteration 34: episode_reward_mean = -168.96
Iteration 35: episode_reward_mean = -171.51
Iteration 36: episode_reward_mean = -174.09
Iteration 37: episode_reward_mean = -178.04
Iteration 38: episode_reward_mean = -163.33
Iteration 39: episode_reward_mean = -164.81
Iteration 40: episode_reward_mean = -170.33
Iteration 41: episode_reward_mean = -169.43
Iteration 42: episode_reward_mean = -168.54
Iteration 43: episode_reward_mean = -168.51
Iteration 44: episode_reward_mean = -175.29
Iteration 45: episode_reward_mean = -176.17
Iteration 46: episode_reward_mean = -176.28
Iteration 47: episode_reward_mean = -176.76
Iteration 48: episode_reward_mean = -181.26
Iteration 49: episode_reward_mean = -181.33
Iteration 50: episode_reward_mean = -178.55
Iteration 51: episode_reward_mean = -177.09
Iteration 52: episode_reward_mean = -177.52
Iteration 53: episode_reward_mean = -176.54
Iteration 54: episode_reward_mean = -174.86
Iteration 55: episode_reward_mean = -170.45
Iteration 56: episode_reward_mean = -175.97
Iteration 57: episode_reward_mean = -174.31
Iteration 58: episode_reward_mean = -179.02
Iteration 59: episode_reward_mean = -185.32
Iteration 60: episode_reward_mean = -182.35
Iteration 61: episode_reward_mean = -179.66
Iteration 62: episode_reward_mean = -184.38
Iteration 63: episode_reward_mean = -187.75
Iteration 64: episode_reward_mean = -181.70
Iteration 65: episode_reward_mean = -183.00
Iteration 66: episode_reward_mean = -185.72
Iteration 67: episode_reward_mean = -186.83
Iteration 68: episode_reward_mean = -183.75
Iteration 69: episode_reward_mean = -186.26
Iteration 70: episode_reward_mean = -185.92
Iteration 71: episode_reward_mean = -186.80
Iteration 72: episode_reward_mean = -201.62
Iteration 73: episode_reward_mean = -203.88
Iteration 74: episode_reward_mean = -205.94
Iteration 75: episode_reward_mean = -213.71
Iteration 76: episode_reward_mean = -215.45
Iteration 77: episode_reward_mean = -219.77
Iteration 78: episode_reward_mean = -218.92
Iteration 79: episode_reward_mean = -209.28
Iteration 80: episode_reward_mean = -217.02
Iteration 81: episode_reward_mean = -225.24
Iteration 82: episode_reward_mean = -223.56
Iteration 83: episode_reward_mean = -227.38
Iteration 84: episode_reward_mean = -229.60
Iteration 85: episode_reward_mean = -229.94
Iteration 86: episode_reward_mean = -229.97
Iteration 87: episode_reward_mean = -237.10
Iteration 88: episode_reward_mean = -237.82
Iteration 89: episode_reward_mean = -239.21
Iteration 90: episode_reward_mean = -240.21
Iteration 91: episode_reward_mean = -239.54
Iteration 92: episode_reward_mean = -230.62
Iteration 93: episode_reward_mean = -236.39
Iteration 94: episode_reward_mean = -238.79
Iteration 95: episode_reward_mean = -238.39
Iteration 96: episode_reward_mean = -235.47
Iteration 97: episode_reward_mean = -236.43
Iteration 98: episode_reward_mean = -230.52
Iteration 99: episode_reward_mean = -237.04
Iteration 100: episode_reward_mean = -229.64
Iteration 101: episode_reward_mean = -225.45
Iteration 102: episode_reward_mean = -220.55
Iteration 103: episode_reward_mean = -218.54
Iteration 104: episode_reward_mean = -221.81
Iteration 105: episode_reward_mean = -224.45
Iteration 106: episode_reward_mean = -225.32
Iteration 107: episode_reward_mean = -222.27
Iteration 108: episode_reward_mean = -224.05
Iteration 109: episode_reward_mean = -225.63
Iteration 110: episode_reward_mean = -229.67
Iteration 111: episode_reward_mean = -232.10
Iteration 112: episode_reward_mean = -231.69
Iteration 113: episode_reward_mean = -222.24
Iteration 114: episode_reward_mean = -221.76
Iteration 115: episode_reward_mean = -223.13
Iteration 116: episode_reward_mean = -224.86
Iteration 117: episode_reward_mean = -223.22
Iteration 118: episode_reward_mean = -229.71
Iteration 119: episode_reward_mean = -231.03
Iteration 120: episode_reward_mean = -234.46
Iteration 121: episode_reward_mean = -233.26
Iteration 122: episode_reward_mean = -234.36
Iteration 123: episode_reward_mean = -233.11
Iteration 124: episode_reward_mean = -233.67
Iteration 125: episode_reward_mean = -233.55
Iteration 126: episode_reward_mean = -235.29
Iteration 127: episode_reward_mean = -232.50
Iteration 128: episode_reward_mean = -236.93
Iteration 129: episode_reward_mean = -235.30
Iteration 130: episode_reward_mean = -233.88
Iteration 131: episode_reward_mean = -233.38
Iteration 132: episode_reward_mean = -228.01
Iteration 133: episode_reward_mean = -233.25
Iteration 134: episode_reward_mean = -236.03
Iteration 135: episode_reward_mean = -232.20
Iteration 136: episode_reward_mean = -230.64
Iteration 137: episode_reward_mean = -228.61
Iteration 138: episode_reward_mean = -225.94
Iteration 139: episode_reward_mean = -223.13
Iteration 140: episode_reward_mean = -222.06
Iteration 141: episode_reward_mean = -222.78
Iteration 142: episode_reward_mean = -230.32
Iteration 143: episode_reward_mean = -231.22
Iteration 144: episode_reward_mean = -227.73
Iteration 145: episode_reward_mean = -227.30
Iteration 146: episode_reward_mean = -224.38
Iteration 147: episode_reward_mean = -236.62
Iteration 148: episode_reward_mean = -233.22
Iteration 149: episode_reward_mean = -235.87
Iteration 150: episode_reward_mean = -237.57
Iteration 151: episode_reward_mean = -238.14
Iteration 152: episode_reward_mean = -244.45
Iteration 153: episode_reward_mean = -241.55
Iteration 154: episode_reward_mean = -234.73
Iteration 155: episode_reward_mean = -235.74
Iteration 156: episode_reward_mean = -233.85
Iteration 157: episode_reward_mean = -237.13
Iteration 158: episode_reward_mean = -240.23
Iteration 159: episode_reward_mean = -244.34
Iteration 160: episode_reward_mean = -248.28
Iteration 161: episode_reward_mean = -251.94
Iteration 162: episode_reward_mean = -251.90
Iteration 163: episode_reward_mean = -252.25
Iteration 164: episode_reward_mean = -252.35
Iteration 165: episode_reward_mean = -252.37
Iteration 166: episode_reward_mean = -252.65
Iteration 167: episode_reward_mean = -242.95
Iteration 168: episode_reward_mean = -242.56
Iteration 169: episode_reward_mean = -237.53
Iteration 170: episode_reward_mean = -229.42
Iteration 171: episode_reward_mean = -223.04
Iteration 172: episode_reward_mean = -217.57
Iteration 173: episode_reward_mean = -221.82
Iteration 174: episode_reward_mean = -225.33
Iteration 175: episode_reward_mean = -226.72
Iteration 176: episode_reward_mean = -229.80
Iteration 177: episode_reward_mean = -225.24
Iteration 178: episode_reward_mean = -226.15
Iteration 179: episode_reward_mean = -222.63
Iteration 180: episode_reward_mean = -220.68
Iteration 181: episode_reward_mean = -221.78
Iteration 182: episode_reward_mean = -218.90
Iteration 183: episode_reward_mean = -219.54
Iteration 184: episode_reward_mean = -223.29
Iteration 185: episode_reward_mean = -223.08
Iteration 186: episode_reward_mean = -227.35
Iteration 187: episode_reward_mean = -230.93
Iteration 188: episode_reward_mean = -232.60
Iteration 189: episode_reward_mean = -234.68
Iteration 190: episode_reward_mean = -242.76
Iteration 191: episode_reward_mean = -252.81
Iteration 192: episode_reward_mean = -263.06
Iteration 193: episode_reward_mean = -258.39
Iteration 194: episode_reward_mean = -255.76
Iteration 195: episode_reward_mean = -257.01
Iteration 196: episode_reward_mean = -258.76
Iteration 197: episode_reward_mean = -261.10
Iteration 198: episode_reward_mean = -257.07
Iteration 199: episode_reward_mean = -254.74
Iteration 200: episode_reward_mean = -255.26
Iteration 201: episode_reward_mean = -251.98
Iteration 202: episode_reward_mean = -253.71
Iteration 203: episode_reward_mean = -249.30
Iteration 204: episode_reward_mean = -249.56
Iteration 205: episode_reward_mean = -248.24
Iteration 206: episode_reward_mean = -246.05
Iteration 207: episode_reward_mean = -240.12
Iteration 208: episode_reward_mean = -238.71
Iteration 209: episode_reward_mean = -235.19
Iteration 210: episode_reward_mean = -235.01
Iteration 211: episode_reward_mean = -232.37
Iteration 212: episode_reward_mean = -222.46
Iteration 213: episode_reward_mean = -231.73
Iteration 214: episode_reward_mean = -238.18
Iteration 215: episode_reward_mean = -241.66
Iteration 216: episode_reward_mean = -244.81
Iteration 217: episode_reward_mean = -249.14
Iteration 218: episode_reward_mean = -253.42
Iteration 219: episode_reward_mean = -256.09
Iteration 220: episode_reward_mean = -260.72
Iteration 221: episode_reward_mean = -264.10
Iteration 222: episode_reward_mean = -264.32
Iteration 223: episode_reward_mean = -273.71
Iteration 224: episode_reward_mean = -272.72
Iteration 225: episode_reward_mean = -275.72
Iteration 226: episode_reward_mean = -278.74
Iteration 227: episode_reward_mean = -287.10
Iteration 228: episode_reward_mean = -288.10
Iteration 229: episode_reward_mean = -296.54
Iteration 230: episode_reward_mean = -299.07
Iteration 231: episode_reward_mean = -304.08
Iteration 232: episode_reward_mean = -305.90
Iteration 233: episode_reward_mean = -302.40
Iteration 234: episode_reward_mean = -301.36
Iteration 235: episode_reward_mean = -300.84
Iteration 236: episode_reward_mean = -293.48
Iteration 237: episode_reward_mean = -288.90
Iteration 238: episode_reward_mean = -290.76
Iteration 239: episode_reward_mean = -299.51
Iteration 240: episode_reward_mean = -297.87
Iteration 241: episode_reward_mean = -297.21
Iteration 242: episode_reward_mean = -296.49
Iteration 243: episode_reward_mean = -297.04
Iteration 244: episode_reward_mean = -298.55
Iteration 245: episode_reward_mean = -302.93
Iteration 246: episode_reward_mean = -304.02
Iteration 247: episode_reward_mean = -296.68
Iteration 248: episode_reward_mean = -298.76
Iteration 249: episode_reward_mean = -294.82
Iteration 250: episode_reward_mean = -287.57
Iteration 251: episode_reward_mean = -282.85
Iteration 252: episode_reward_mean = -289.58
Iteration 253: episode_reward_mean = -291.95
Iteration 254: episode_reward_mean = -289.97
Iteration 255: episode_reward_mean = -288.29
Iteration 256: episode_reward_mean = -290.88
Iteration 257: episode_reward_mean = -294.90
Iteration 258: episode_reward_mean = -294.57
Iteration 259: episode_reward_mean = -283.02
Iteration 260: episode_reward_mean = -281.11
Iteration 261: episode_reward_mean = -284.45
Iteration 262: episode_reward_mean = -287.28
Iteration 263: episode_reward_mean = -279.77
Iteration 264: episode_reward_mean = -278.63
Iteration 265: episode_reward_mean = -277.45
Iteration 266: episode_reward_mean = -272.00
Iteration 267: episode_reward_mean = -274.93
Iteration 268: episode_reward_mean = -270.21
Iteration 269: episode_reward_mean = -271.43
Iteration 270: episode_reward_mean = -273.91
Iteration 271: episode_reward_mean = -273.19
Iteration 272: episode_reward_mean = -272.64
Iteration 273: episode_reward_mean = -266.45
Iteration 274: episode_reward_mean = -264.48
Iteration 275: episode_reward_mean = -258.14
Iteration 276: episode_reward_mean = -257.81
Iteration 277: episode_reward_mean = -255.77
Iteration 278: episode_reward_mean = -248.70
Iteration 279: episode_reward_mean = -251.14
Iteration 280: episode_reward_mean = -247.73
Iteration 281: episode_reward_mean = -248.69
Iteration 282: episode_reward_mean = -247.09
Iteration 283: episode_reward_mean = -248.79
Iteration 284: episode_reward_mean = -253.84
Iteration 285: episode_reward_mean = -252.08
Iteration 286: episode_reward_mean = -247.76
Iteration 287: episode_reward_mean = -248.62
Iteration 288: episode_reward_mean = -248.12
Iteration 289: episode_reward_mean = -243.91
Iteration 290: episode_reward_mean = -238.84
Iteration 291: episode_reward_mean = -242.24
Iteration 292: episode_reward_mean = -238.02
Iteration 293: episode_reward_mean = -238.38
Iteration 294: episode_reward_mean = -238.67
Iteration 295: episode_reward_mean = -242.84
Iteration 296: episode_reward_mean = -245.64
Iteration 297: episode_reward_mean = -250.07
Iteration 298: episode_reward_mean = -263.79
Iteration 299: episode_reward_mean = -274.91
Iteration 300: episode_reward_mean = -284.84

[INFO] Checkpoint saved at: /Users/ali/ray_results/DQN_RandomizedPendulum-v1_2025-07-08_15-14-25i_iesphs/checkpoint_000300

[INFO] Training Complete
Avg Training CPU Usage: 18.13%
Peak Memory Usage: 555.03 MB
Final Memory Change: 172.44 MB
Training Time: 629.09 seconds

Running trained DQN policy on RandomizedPendulum-v1 and logging state evolution...

2025-07-08 15:24:54,708	WARNING util.py:66 -- Install gputil for GPU system monitoring.
2025-07-08 15:24:54,738	INFO trainable.py:766 -- Restored on 127.0.0.1 from checkpoint: /Users/ali/ray_results/DQN_RandomizedPendulum-v1_2025-07-08_15-14-25i_iesphs/checkpoint_000300
2025-07-08 15:24:54,738	INFO trainable.py:775 -- Current state after restoring: {'_iteration': 300, '_timesteps_total': None, '_time_total': 625.6675696372986, '_episodes_total': 1500}
Inference log saved to pendulum_inference_DQN_log.csv

State plot saved to DQN-Pendulum-v1.png