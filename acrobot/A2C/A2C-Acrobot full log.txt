(avis) ali@Mac A2C % python a2c.py 
/Users/ali/miniforge3/envs/avis/lib/python3.10/site-packages/ray/air/_internal/remote_storage.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging
2025-07-08 16:48:01,684	INFO worker.py:1528 -- Started a local Ray instance.

[INFO] Starting A2C training on RandomizedAcrobot-v1 with RANDOMIZED INITIAL STATES...

(raylet) /Users/ali/miniforge3/envs/avis/lib/python3.10/site-packages/ray/air/_internal/remote_storage.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
(raylet)   from pkg_resources import packaging
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1751986082.363236 2644441 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers
2025-07-08 16:48:02,372	WARNING a2c.py:144 -- `train_batch_size` (32) cannot be smaller than sample_batch_size (`rollout_fragment_length` x `num_workers` x `num_envs_per_worker`) (40) when micro-batching is not set. This is to ensure that only on gradient update is applied to policy in every iteration on the entire collected batch. As a result of we do not change the policy too much before we sample again and stay on policy as much as possible. This will help the learning stability. Setting train_batch_size = 40.
2025-07-08 16:48:02,373	INFO algorithm.py:457 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
(raylet) /Users/ali/miniforge3/envs/avis/lib/python3.10/site-packages/ray/air/_internal/remote_storage.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
(raylet)   from pkg_resources import packaging
(raylet) /Users/ali/miniforge3/envs/avis/lib/python3.10/site-packages/ray/air/_internal/remote_storage.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
(raylet)   from pkg_resources import packaging
2025-07-08 16:48:05,538	WARNING util.py:66 -- Install gputil for GPU system monitoring.
Iteration 1: episode_reward_mean = -261.44
Iteration 2: episode_reward_mean = -266.16
Iteration 3: episode_reward_mean = -221.39
Iteration 4: episode_reward_mean = -260.05
Iteration 5: episode_reward_mean = -234.89
Iteration 6: episode_reward_mean = -186.81
Iteration 7: episode_reward_mean = -192.35
Iteration 8: episode_reward_mean = -196.10
Iteration 9: episode_reward_mean = -217.31
Iteration 10: episode_reward_mean = -212.78
Iteration 11: episode_reward_mean = -189.00
Iteration 12: episode_reward_mean = -164.97
Iteration 13: episode_reward_mean = -162.36
Iteration 14: episode_reward_mean = -168.94
Iteration 15: episode_reward_mean = -148.94
Iteration 16: episode_reward_mean = -160.05
Iteration 17: episode_reward_mean = -147.34
Iteration 18: episode_reward_mean = -74.91
Iteration 19: episode_reward_mean = -54.46
Iteration 20: episode_reward_mean = -47.30
Iteration 21: episode_reward_mean = -48.30
Iteration 22: episode_reward_mean = -49.60
Iteration 23: episode_reward_mean = -44.82
Iteration 24: episode_reward_mean = -49.58
Iteration 25: episode_reward_mean = -51.33
Iteration 26: episode_reward_mean = -46.78
Iteration 27: episode_reward_mean = -45.78
Iteration 28: episode_reward_mean = -46.14
Iteration 29: episode_reward_mean = -48.80
Iteration 30: episode_reward_mean = -49.99
Iteration 31: episode_reward_mean = -46.30
Iteration 32: episode_reward_mean = -42.15
Iteration 33: episode_reward_mean = -44.70
Iteration 34: episode_reward_mean = -48.94
Iteration 35: episode_reward_mean = -43.40
Iteration 36: episode_reward_mean = -40.69
Iteration 37: episode_reward_mean = -44.23
Iteration 38: episode_reward_mean = -42.42
Iteration 39: episode_reward_mean = -43.73
Iteration 40: episode_reward_mean = -44.16
Iteration 41: episode_reward_mean = -43.16
Iteration 42: episode_reward_mean = -41.17
Iteration 43: episode_reward_mean = -41.33
Iteration 44: episode_reward_mean = -42.56
Iteration 45: episode_reward_mean = -40.33
Iteration 46: episode_reward_mean = -42.77
Iteration 47: episode_reward_mean = -41.64
Iteration 48: episode_reward_mean = -43.62
Iteration 49: episode_reward_mean = -48.06
Iteration 50: episode_reward_mean = -40.78
Iteration 51: episode_reward_mean = -43.11
Iteration 52: episode_reward_mean = -41.71
Iteration 53: episode_reward_mean = -44.21
Iteration 54: episode_reward_mean = -41.67
Iteration 55: episode_reward_mean = -40.98
Iteration 56: episode_reward_mean = -42.41
Iteration 57: episode_reward_mean = -41.69
Iteration 58: episode_reward_mean = -39.55
Iteration 59: episode_reward_mean = -41.30
Iteration 60: episode_reward_mean = -42.38
Iteration 61: episode_reward_mean = -41.56
Iteration 62: episode_reward_mean = -41.89
Iteration 63: episode_reward_mean = -40.76
Iteration 64: episode_reward_mean = -40.77
Iteration 65: episode_reward_mean = -39.95
Iteration 66: episode_reward_mean = -41.74
Iteration 67: episode_reward_mean = -40.80
Iteration 68: episode_reward_mean = -43.99
Iteration 69: episode_reward_mean = -42.21
Iteration 70: episode_reward_mean = -44.63
Iteration 71: episode_reward_mean = -41.86
Iteration 72: episode_reward_mean = -42.78
Iteration 73: episode_reward_mean = -41.37
Iteration 74: episode_reward_mean = -43.07
Iteration 75: episode_reward_mean = -42.57
Iteration 76: episode_reward_mean = -43.84
Iteration 77: episode_reward_mean = -42.61
Iteration 78: episode_reward_mean = -41.18
Iteration 79: episode_reward_mean = -44.33
Iteration 80: episode_reward_mean = -39.30
Iteration 81: episode_reward_mean = -42.45
Iteration 82: episode_reward_mean = -40.57
Iteration 83: episode_reward_mean = -43.65
Iteration 84: episode_reward_mean = -45.15
Iteration 85: episode_reward_mean = -41.38
Iteration 86: episode_reward_mean = -45.39
Iteration 87: episode_reward_mean = -40.97
Iteration 88: episode_reward_mean = -39.36
Iteration 89: episode_reward_mean = -39.63
Iteration 90: episode_reward_mean = -41.01
Iteration 91: episode_reward_mean = -39.76
Iteration 92: episode_reward_mean = -39.65
Iteration 93: episode_reward_mean = -41.75
Iteration 94: episode_reward_mean = -43.94
Iteration 95: episode_reward_mean = -47.07
Iteration 96: episode_reward_mean = -42.50
Iteration 97: episode_reward_mean = -44.65
Iteration 98: episode_reward_mean = -43.44
Iteration 99: episode_reward_mean = -40.28
Iteration 100: episode_reward_mean = -44.90
Iteration 101: episode_reward_mean = -39.38
Iteration 102: episode_reward_mean = -41.45
Iteration 103: episode_reward_mean = -41.70
Iteration 104: episode_reward_mean = -42.62
Iteration 105: episode_reward_mean = -42.71
Iteration 106: episode_reward_mean = -46.74
Iteration 107: episode_reward_mean = -41.38
Iteration 108: episode_reward_mean = -40.17
Iteration 109: episode_reward_mean = -43.90
Iteration 110: episode_reward_mean = -42.42
Iteration 111: episode_reward_mean = -41.76
Iteration 112: episode_reward_mean = -40.24
Iteration 113: episode_reward_mean = -42.00
Iteration 114: episode_reward_mean = -39.49
Iteration 115: episode_reward_mean = -38.68
Iteration 116: episode_reward_mean = -38.38
Iteration 117: episode_reward_mean = -45.02
Iteration 118: episode_reward_mean = -40.90
Iteration 119: episode_reward_mean = -38.84
Iteration 120: episode_reward_mean = -42.92
Iteration 121: episode_reward_mean = -37.47
Iteration 122: episode_reward_mean = -43.24
Iteration 123: episode_reward_mean = -40.11
Iteration 124: episode_reward_mean = -39.91
Iteration 125: episode_reward_mean = -40.46
Iteration 126: episode_reward_mean = -39.35
Iteration 127: episode_reward_mean = -42.62
Iteration 128: episode_reward_mean = -39.06
Iteration 129: episode_reward_mean = -40.40
Iteration 130: episode_reward_mean = -40.79
Iteration 131: episode_reward_mean = -38.31
Iteration 132: episode_reward_mean = -40.49
Iteration 133: episode_reward_mean = -39.44
Iteration 134: episode_reward_mean = -38.82
Iteration 135: episode_reward_mean = -40.68
Iteration 136: episode_reward_mean = -39.73
Iteration 137: episode_reward_mean = -42.39
Iteration 138: episode_reward_mean = -36.75
Iteration 139: episode_reward_mean = -41.26
Iteration 140: episode_reward_mean = -36.09
Iteration 141: episode_reward_mean = -42.18
Iteration 142: episode_reward_mean = -40.49
Iteration 143: episode_reward_mean = -40.08
Iteration 144: episode_reward_mean = -37.95
Iteration 145: episode_reward_mean = -36.75
Iteration 146: episode_reward_mean = -37.92
Iteration 147: episode_reward_mean = -39.55
Iteration 148: episode_reward_mean = -39.26
Iteration 149: episode_reward_mean = -42.80
Iteration 150: episode_reward_mean = -44.42
Iteration 151: episode_reward_mean = -44.21
Iteration 152: episode_reward_mean = -40.20
Iteration 153: episode_reward_mean = -40.84
Iteration 154: episode_reward_mean = -38.60
Iteration 155: episode_reward_mean = -40.00
Iteration 156: episode_reward_mean = -37.89
Iteration 157: episode_reward_mean = -41.19
Iteration 158: episode_reward_mean = -40.31
Iteration 159: episode_reward_mean = -36.97
Iteration 160: episode_reward_mean = -41.26
Iteration 161: episode_reward_mean = -41.26
Iteration 162: episode_reward_mean = -43.82
Iteration 163: episode_reward_mean = -38.86
Iteration 164: episode_reward_mean = -39.32
Iteration 165: episode_reward_mean = -39.01
Iteration 166: episode_reward_mean = -42.73
Iteration 167: episode_reward_mean = -39.63
Iteration 168: episode_reward_mean = -42.70
Iteration 169: episode_reward_mean = -40.06
Iteration 170: episode_reward_mean = -40.37
Iteration 171: episode_reward_mean = -38.52
Iteration 172: episode_reward_mean = -41.51
Iteration 173: episode_reward_mean = -37.65
Iteration 174: episode_reward_mean = -41.32
Iteration 175: episode_reward_mean = -39.66
Iteration 176: episode_reward_mean = -40.41
Iteration 177: episode_reward_mean = -44.23
Iteration 178: episode_reward_mean = -37.68
Iteration 179: episode_reward_mean = -37.45
Iteration 180: episode_reward_mean = -40.73
Iteration 181: episode_reward_mean = -44.30
Iteration 182: episode_reward_mean = -39.68
Iteration 183: episode_reward_mean = -40.51
Iteration 184: episode_reward_mean = -39.09
Iteration 185: episode_reward_mean = -39.98
Iteration 186: episode_reward_mean = -39.28
Iteration 187: episode_reward_mean = -39.80
Iteration 188: episode_reward_mean = -39.83
Iteration 189: episode_reward_mean = -38.87
Iteration 190: episode_reward_mean = -39.48
Iteration 191: episode_reward_mean = -41.29
Iteration 192: episode_reward_mean = -43.96
Iteration 193: episode_reward_mean = -40.89
Iteration 194: episode_reward_mean = -38.59
Iteration 195: episode_reward_mean = -41.85
Iteration 196: episode_reward_mean = -39.53
Iteration 197: episode_reward_mean = -37.68
Iteration 198: episode_reward_mean = -39.79
Iteration 199: episode_reward_mean = -39.30
Iteration 200: episode_reward_mean = -38.99
Iteration 201: episode_reward_mean = -39.99
Iteration 202: episode_reward_mean = -43.12
Iteration 203: episode_reward_mean = -38.64
Iteration 204: episode_reward_mean = -40.72
Iteration 205: episode_reward_mean = -39.79
Iteration 206: episode_reward_mean = -40.06
Iteration 207: episode_reward_mean = -37.96
Iteration 208: episode_reward_mean = -42.85
Iteration 209: episode_reward_mean = -36.33
Iteration 210: episode_reward_mean = -42.48
Iteration 211: episode_reward_mean = -37.52
Iteration 212: episode_reward_mean = -37.28
Iteration 213: episode_reward_mean = -36.80
Iteration 214: episode_reward_mean = -39.68
Iteration 215: episode_reward_mean = -37.58
Iteration 216: episode_reward_mean = -38.58
Iteration 217: episode_reward_mean = -40.61
Iteration 218: episode_reward_mean = -40.48
Iteration 219: episode_reward_mean = -41.62
Iteration 220: episode_reward_mean = -37.54
Iteration 221: episode_reward_mean = -42.03
Iteration 222: episode_reward_mean = -40.21
Iteration 223: episode_reward_mean = -39.40
Iteration 224: episode_reward_mean = -35.56
Iteration 225: episode_reward_mean = -39.49
Iteration 226: episode_reward_mean = -39.27
Iteration 227: episode_reward_mean = -37.86
Iteration 228: episode_reward_mean = -39.70
Iteration 229: episode_reward_mean = -37.71
Iteration 230: episode_reward_mean = -39.20
Iteration 231: episode_reward_mean = -38.49
Iteration 232: episode_reward_mean = -37.88
Iteration 233: episode_reward_mean = -35.92
Iteration 234: episode_reward_mean = -36.05
Iteration 235: episode_reward_mean = -40.56
Iteration 236: episode_reward_mean = -36.91
Iteration 237: episode_reward_mean = -36.52
Iteration 238: episode_reward_mean = -36.68
Iteration 239: episode_reward_mean = -39.25
Iteration 240: episode_reward_mean = -38.26
Iteration 241: episode_reward_mean = -37.36
Iteration 242: episode_reward_mean = -42.14
Iteration 243: episode_reward_mean = -37.40
Iteration 244: episode_reward_mean = -34.82
Iteration 245: episode_reward_mean = -37.09
Iteration 246: episode_reward_mean = -38.80
Iteration 247: episode_reward_mean = -37.80
Iteration 248: episode_reward_mean = -36.93
Iteration 249: episode_reward_mean = -39.53
Iteration 250: episode_reward_mean = -38.01
Iteration 251: episode_reward_mean = -39.18
Iteration 252: episode_reward_mean = -37.23
Iteration 253: episode_reward_mean = -38.14
Iteration 254: episode_reward_mean = -37.04
Iteration 255: episode_reward_mean = -35.03
Iteration 256: episode_reward_mean = -38.64
Iteration 257: episode_reward_mean = -38.36
Iteration 258: episode_reward_mean = -36.71
Iteration 259: episode_reward_mean = -37.79
Iteration 260: episode_reward_mean = -39.11
Iteration 261: episode_reward_mean = -38.84
Iteration 262: episode_reward_mean = -38.53
Iteration 263: episode_reward_mean = -34.87
Iteration 264: episode_reward_mean = -36.92
Iteration 265: episode_reward_mean = -35.94
Iteration 266: episode_reward_mean = -37.87
Iteration 267: episode_reward_mean = -34.69
Iteration 268: episode_reward_mean = -36.08
Iteration 269: episode_reward_mean = -36.43
Iteration 270: episode_reward_mean = -34.62
Iteration 271: episode_reward_mean = -40.41
Iteration 272: episode_reward_mean = -36.03
Iteration 273: episode_reward_mean = -37.67
Iteration 274: episode_reward_mean = -37.34
Iteration 275: episode_reward_mean = -36.63
Iteration 276: episode_reward_mean = -37.00
Iteration 277: episode_reward_mean = -37.40
Iteration 278: episode_reward_mean = -37.69
Iteration 279: episode_reward_mean = -37.24
Iteration 280: episode_reward_mean = -37.43
Iteration 281: episode_reward_mean = -41.39
Iteration 282: episode_reward_mean = -37.33
Iteration 283: episode_reward_mean = -38.40
Iteration 284: episode_reward_mean = -37.34
Iteration 285: episode_reward_mean = -38.33
Iteration 286: episode_reward_mean = -35.43
Iteration 287: episode_reward_mean = -36.14
Iteration 288: episode_reward_mean = -35.24
Iteration 289: episode_reward_mean = -38.99
Iteration 290: episode_reward_mean = -36.41
Iteration 291: episode_reward_mean = -37.26
Iteration 292: episode_reward_mean = -34.48
Iteration 293: episode_reward_mean = -37.20
Iteration 294: episode_reward_mean = -37.47
Iteration 295: episode_reward_mean = -37.27
Iteration 296: episode_reward_mean = -37.89
Iteration 297: episode_reward_mean = -34.80
Iteration 298: episode_reward_mean = -35.31
Iteration 299: episode_reward_mean = -36.33
Iteration 300: episode_reward_mean = -34.26

[INFO] Checkpoint saved at: /Users/ali/ray_results/A2C_RandomizedAcrobot-v1_2025-07-08_16-48-02vsuvz2is/checkpoint_000300

[INFO] Training Complete
Avg Training CPU Usage: 32.61%
Peak Memory Usage: 447.31 MB
Final Memory Change: 39.61 MB
Training Time: 3009.65 seconds

Running trained A2C policy on RandomizedAcrobot-v1 and logging state evolution...

2025-07-08 17:38:12,015	WARNING a2c.py:144 -- `train_batch_size` (32) cannot be smaller than sample_batch_size (`rollout_fragment_length` x `num_workers` x `num_envs_per_worker`) (40) when micro-batching is not set. This is to ensure that only on gradient update is applied to policy in every iteration on the entire collected batch. As a result of we do not change the policy too much before we sample again and stay on policy as much as possible. This will help the learning stability. Setting train_batch_size = 40.
(raylet) /Users/ali/miniforge3/envs/avis/lib/python3.10/site-packages/ray/air/_internal/remote_storage.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
(raylet)   from pkg_resources import packaging
(raylet) /Users/ali/miniforge3/envs/avis/lib/python3.10/site-packages/ray/air/_internal/remote_storage.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
(raylet)   from pkg_resources import packaging
2025-07-08 17:38:15,636	WARNING util.py:66 -- Install gputil for GPU system monitoring.
2025-07-08 17:38:15,639	INFO trainable.py:766 -- Restored on 127.0.0.1 from checkpoint: /Users/ali/ray_results/A2C_RandomizedAcrobot-v1_2025-07-08_16-48-02vsuvz2is/checkpoint_000300
2025-07-08 17:38:15,639	INFO trainable.py:775 -- Current state after restoring: {'_iteration': 300, '_timesteps_total': None, '_time_total': 3003.2867352962494, '_episodes_total': 207676}
Inference log saved to acrobot_inference_A2C_log.csv

State plot saved to A2C-Acrobot-v1.png

