(avis) ali@Mac DQN % python dqn.py 
/Users/ali/miniforge3/envs/avis/lib/python3.10/site-packages/ray/air/_internal/remote_storage.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging
2025-07-08 16:32:44,295	INFO worker.py:1528 -- Started a local Ray instance.
(raylet) /Users/ali/miniforge3/envs/avis/lib/python3.10/site-packages/ray/air/_internal/remote_storage.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
(raylet)   from pkg_resources import packaging

[INFO] Starting DQN training on RandomizedAcrobot-v1 with RANDOMIZED INITIAL STATES...

WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1751985164.989685 2634389 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers
2025-07-08 16:32:44,998	INFO simple_q.py:307 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting `simple_optimizer=True` if this doesn't work for you.
2025-07-08 16:32:44,999	INFO algorithm.py:457 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
2025-07-08 16:32:45,833	WARNING util.py:66 -- Install gputil for GPU system monitoring.
2025-07-08 16:32:45,850	WARNING multi_agent_prioritized_replay_buffer.py:215 -- Adding batches with column `weights` to this buffer while providing weights as a call argument to the add method results in the column being overwritten.
Iteration 1: episode_reward_mean = -413.50
2025-07-08 16:32:46,443	WARNING deprecation.py:47 -- DeprecationWarning: `concat_samples` has been deprecated. Use `concat_samples() from rllib.policy.sample_batch` instead. This will raise an error in the future!
Iteration 2: episode_reward_mean = -329.33
Iteration 3: episode_reward_mean = -248.00
Iteration 4: episode_reward_mean = -284.00
Iteration 5: episode_reward_mean = -275.59
Iteration 6: episode_reward_mean = -283.48
Iteration 7: episode_reward_mean = -302.30
Iteration 8: episode_reward_mean = -280.07
Iteration 9: episode_reward_mean = -246.36
Iteration 10: episode_reward_mean = -191.00
Iteration 11: episode_reward_mean = -161.92
Iteration 12: episode_reward_mean = -150.63
Iteration 13: episode_reward_mean = -147.94
Iteration 14: episode_reward_mean = -117.01
Iteration 15: episode_reward_mean = -94.38
Iteration 16: episode_reward_mean = -69.34
Iteration 17: episode_reward_mean = -67.88
Iteration 18: episode_reward_mean = -72.36
Iteration 19: episode_reward_mean = -62.77
Iteration 20: episode_reward_mean = -67.81
Iteration 21: episode_reward_mean = -62.29
Iteration 22: episode_reward_mean = -65.68
Iteration 23: episode_reward_mean = -65.31
Iteration 24: episode_reward_mean = -58.71
Iteration 25: episode_reward_mean = -47.34
Iteration 26: episode_reward_mean = -46.05
Iteration 27: episode_reward_mean = -42.02
Iteration 28: episode_reward_mean = -40.24
Iteration 29: episode_reward_mean = -43.37
Iteration 30: episode_reward_mean = -46.11
Iteration 31: episode_reward_mean = -45.32
Iteration 32: episode_reward_mean = -42.08
Iteration 33: episode_reward_mean = -40.13
Iteration 34: episode_reward_mean = -38.97
Iteration 35: episode_reward_mean = -40.09
Iteration 36: episode_reward_mean = -39.00
Iteration 37: episode_reward_mean = -35.86
Iteration 38: episode_reward_mean = -28.63
Iteration 39: episode_reward_mean = -28.87
Iteration 40: episode_reward_mean = -29.48
Iteration 41: episode_reward_mean = -33.67
Iteration 42: episode_reward_mean = -37.34
Iteration 43: episode_reward_mean = -37.20
Iteration 44: episode_reward_mean = -38.82
Iteration 45: episode_reward_mean = -38.90
Iteration 46: episode_reward_mean = -37.47
Iteration 47: episode_reward_mean = -37.28
Iteration 48: episode_reward_mean = -36.71
Iteration 49: episode_reward_mean = -36.30
Iteration 50: episode_reward_mean = -35.98
Iteration 51: episode_reward_mean = -38.10
Iteration 52: episode_reward_mean = -37.60
Iteration 53: episode_reward_mean = -35.30
Iteration 54: episode_reward_mean = -36.47
Iteration 55: episode_reward_mean = -35.11
Iteration 56: episode_reward_mean = -32.48
Iteration 57: episode_reward_mean = -33.04
Iteration 58: episode_reward_mean = -36.41
Iteration 59: episode_reward_mean = -37.50
Iteration 60: episode_reward_mean = -38.97
Iteration 61: episode_reward_mean = -38.60
Iteration 62: episode_reward_mean = -35.86
Iteration 63: episode_reward_mean = -34.67
Iteration 64: episode_reward_mean = -33.00
Iteration 65: episode_reward_mean = -34.88
Iteration 66: episode_reward_mean = -35.72
Iteration 67: episode_reward_mean = -36.23
Iteration 68: episode_reward_mean = -34.64
Iteration 69: episode_reward_mean = -32.74
Iteration 70: episode_reward_mean = -31.76
Iteration 71: episode_reward_mean = -34.62
Iteration 72: episode_reward_mean = -37.20
Iteration 73: episode_reward_mean = -39.19
Iteration 74: episode_reward_mean = -35.09
Iteration 75: episode_reward_mean = -33.36
Iteration 76: episode_reward_mean = -31.35
Iteration 77: episode_reward_mean = -31.94
Iteration 78: episode_reward_mean = -35.06
Iteration 79: episode_reward_mean = -33.59
Iteration 80: episode_reward_mean = -31.84
Iteration 81: episode_reward_mean = -29.86
Iteration 82: episode_reward_mean = -28.25
Iteration 83: episode_reward_mean = -28.60
Iteration 84: episode_reward_mean = -29.58
Iteration 85: episode_reward_mean = -28.89
Iteration 86: episode_reward_mean = -29.33
Iteration 87: episode_reward_mean = -28.78
Iteration 88: episode_reward_mean = -29.83
Iteration 89: episode_reward_mean = -30.03
Iteration 90: episode_reward_mean = -30.78
Iteration 91: episode_reward_mean = -31.71
Iteration 92: episode_reward_mean = -35.43
Iteration 93: episode_reward_mean = -36.02
Iteration 94: episode_reward_mean = -38.56
Iteration 95: episode_reward_mean = -36.95
Iteration 96: episode_reward_mean = -35.03
Iteration 97: episode_reward_mean = -30.80
Iteration 98: episode_reward_mean = -29.99
Iteration 99: episode_reward_mean = -31.22
Iteration 100: episode_reward_mean = -33.03
Iteration 101: episode_reward_mean = -35.13
Iteration 102: episode_reward_mean = -32.85
Iteration 103: episode_reward_mean = -31.64
Iteration 104: episode_reward_mean = -28.85
Iteration 105: episode_reward_mean = -32.02
Iteration 106: episode_reward_mean = -30.38
Iteration 107: episode_reward_mean = -30.44
Iteration 108: episode_reward_mean = -32.07
Iteration 109: episode_reward_mean = -33.34
Iteration 110: episode_reward_mean = -34.86
Iteration 111: episode_reward_mean = -34.85
Iteration 112: episode_reward_mean = -29.98
Iteration 113: episode_reward_mean = -30.48
Iteration 114: episode_reward_mean = -31.13
Iteration 115: episode_reward_mean = -30.64
Iteration 116: episode_reward_mean = -33.86
Iteration 117: episode_reward_mean = -34.92
Iteration 118: episode_reward_mean = -33.84
Iteration 119: episode_reward_mean = -34.40
Iteration 120: episode_reward_mean = -28.56
Iteration 121: episode_reward_mean = -29.46
Iteration 122: episode_reward_mean = -30.27
Iteration 123: episode_reward_mean = -30.90
Iteration 124: episode_reward_mean = -33.65
Iteration 125: episode_reward_mean = -36.50
Iteration 126: episode_reward_mean = -34.35
Iteration 127: episode_reward_mean = -33.42
Iteration 128: episode_reward_mean = -32.46
Iteration 129: episode_reward_mean = -33.11
Iteration 130: episode_reward_mean = -30.60
Iteration 131: episode_reward_mean = -33.02
Iteration 132: episode_reward_mean = -32.82
Iteration 133: episode_reward_mean = -33.66
Iteration 134: episode_reward_mean = -31.06
Iteration 135: episode_reward_mean = -31.91
Iteration 136: episode_reward_mean = -32.19
Iteration 137: episode_reward_mean = -35.47
Iteration 138: episode_reward_mean = -34.02
Iteration 139: episode_reward_mean = -36.64
Iteration 140: episode_reward_mean = -36.99
Iteration 141: episode_reward_mean = -36.07
Iteration 142: episode_reward_mean = -34.48
Iteration 143: episode_reward_mean = -32.13
Iteration 144: episode_reward_mean = -29.21
Iteration 145: episode_reward_mean = -32.62
Iteration 146: episode_reward_mean = -32.64
Iteration 147: episode_reward_mean = -35.37
Iteration 148: episode_reward_mean = -34.19
Iteration 149: episode_reward_mean = -33.40
Iteration 150: episode_reward_mean = -34.81
Iteration 151: episode_reward_mean = -31.78
Iteration 152: episode_reward_mean = -28.69
Iteration 153: episode_reward_mean = -24.90
Iteration 154: episode_reward_mean = -26.63
Iteration 155: episode_reward_mean = -31.47
Iteration 156: episode_reward_mean = -32.42
Iteration 157: episode_reward_mean = -34.06
Iteration 158: episode_reward_mean = -35.07
Iteration 159: episode_reward_mean = -32.25
Iteration 160: episode_reward_mean = -29.67
Iteration 161: episode_reward_mean = -29.48
Iteration 162: episode_reward_mean = -29.91
Iteration 163: episode_reward_mean = -31.01
Iteration 164: episode_reward_mean = -30.59
Iteration 165: episode_reward_mean = -31.43
Iteration 166: episode_reward_mean = -32.58
Iteration 167: episode_reward_mean = -30.99
Iteration 168: episode_reward_mean = -29.16
Iteration 169: episode_reward_mean = -27.88
Iteration 170: episode_reward_mean = -27.74
Iteration 171: episode_reward_mean = -30.07
Iteration 172: episode_reward_mean = -29.95
Iteration 173: episode_reward_mean = -33.04
Iteration 174: episode_reward_mean = -36.07
Iteration 175: episode_reward_mean = -35.58
Iteration 176: episode_reward_mean = -32.85
Iteration 177: episode_reward_mean = -34.02
Iteration 178: episode_reward_mean = -29.71
Iteration 179: episode_reward_mean = -32.29
Iteration 180: episode_reward_mean = -32.14
Iteration 181: episode_reward_mean = -34.04
Iteration 182: episode_reward_mean = -34.99
Iteration 183: episode_reward_mean = -33.12
Iteration 184: episode_reward_mean = -33.10
Iteration 185: episode_reward_mean = -32.83
Iteration 186: episode_reward_mean = -33.21
Iteration 187: episode_reward_mean = -33.74
Iteration 188: episode_reward_mean = -33.38
Iteration 189: episode_reward_mean = -33.72
Iteration 190: episode_reward_mean = -31.31
Iteration 191: episode_reward_mean = -32.17
Iteration 192: episode_reward_mean = -32.86
Iteration 193: episode_reward_mean = -30.94
Iteration 194: episode_reward_mean = -30.00
Iteration 195: episode_reward_mean = -27.83
Iteration 196: episode_reward_mean = -28.61
Iteration 197: episode_reward_mean = -28.47
Iteration 198: episode_reward_mean = -29.56
Iteration 199: episode_reward_mean = -31.64
Iteration 200: episode_reward_mean = -29.49
Iteration 201: episode_reward_mean = -29.48
Iteration 202: episode_reward_mean = -30.37
Iteration 203: episode_reward_mean = -32.42
Iteration 204: episode_reward_mean = -33.36
Iteration 205: episode_reward_mean = -31.92
Iteration 206: episode_reward_mean = -32.98
Iteration 207: episode_reward_mean = -34.12
Iteration 208: episode_reward_mean = -33.97
Iteration 209: episode_reward_mean = -31.11
Iteration 210: episode_reward_mean = -30.54
Iteration 211: episode_reward_mean = -31.34
Iteration 212: episode_reward_mean = -32.35
Iteration 213: episode_reward_mean = -32.23
Iteration 214: episode_reward_mean = -33.33
Iteration 215: episode_reward_mean = -31.41
Iteration 216: episode_reward_mean = -28.94
Iteration 217: episode_reward_mean = -25.53
Iteration 218: episode_reward_mean = -24.32
Iteration 219: episode_reward_mean = -27.10
Iteration 220: episode_reward_mean = -30.00
Iteration 221: episode_reward_mean = -30.07
Iteration 222: episode_reward_mean = -26.48
Iteration 223: episode_reward_mean = -27.40
Iteration 224: episode_reward_mean = -30.95
Iteration 225: episode_reward_mean = -33.37
Iteration 226: episode_reward_mean = -33.74
Iteration 227: episode_reward_mean = -34.22
Iteration 228: episode_reward_mean = -32.65
Iteration 229: episode_reward_mean = -31.30
Iteration 230: episode_reward_mean = -28.29
Iteration 231: episode_reward_mean = -27.75
Iteration 232: episode_reward_mean = -26.72
Iteration 233: episode_reward_mean = -27.03
Iteration 234: episode_reward_mean = -26.94
Iteration 235: episode_reward_mean = -28.10
Iteration 236: episode_reward_mean = -28.16
Iteration 237: episode_reward_mean = -27.82
Iteration 238: episode_reward_mean = -27.52
Iteration 239: episode_reward_mean = -28.92
Iteration 240: episode_reward_mean = -32.58
Iteration 241: episode_reward_mean = -32.92
Iteration 242: episode_reward_mean = -33.04
Iteration 243: episode_reward_mean = -31.08
Iteration 244: episode_reward_mean = -29.25
Iteration 245: episode_reward_mean = -25.72
Iteration 246: episode_reward_mean = -26.14
Iteration 247: episode_reward_mean = -25.06
Iteration 248: episode_reward_mean = -23.85
Iteration 249: episode_reward_mean = -25.45
Iteration 250: episode_reward_mean = -26.57
Iteration 251: episode_reward_mean = -27.74
Iteration 252: episode_reward_mean = -29.91
Iteration 253: episode_reward_mean = -31.00
Iteration 254: episode_reward_mean = -29.91
Iteration 255: episode_reward_mean = -31.00
Iteration 256: episode_reward_mean = -26.19
Iteration 257: episode_reward_mean = -29.09
Iteration 258: episode_reward_mean = -28.30
Iteration 259: episode_reward_mean = -30.92
Iteration 260: episode_reward_mean = -27.56
Iteration 261: episode_reward_mean = -25.94
Iteration 262: episode_reward_mean = -27.91
Iteration 263: episode_reward_mean = -29.27
Iteration 264: episode_reward_mean = -31.91
Iteration 265: episode_reward_mean = -32.92
Iteration 266: episode_reward_mean = -33.26
Iteration 267: episode_reward_mean = -33.46
Iteration 268: episode_reward_mean = -31.47
Iteration 269: episode_reward_mean = -30.35
Iteration 270: episode_reward_mean = -30.09
Iteration 271: episode_reward_mean = -30.34
Iteration 272: episode_reward_mean = -29.81
Iteration 273: episode_reward_mean = -28.62
Iteration 274: episode_reward_mean = -28.80
Iteration 275: episode_reward_mean = -29.96
Iteration 276: episode_reward_mean = -28.87
Iteration 277: episode_reward_mean = -29.67
Iteration 278: episode_reward_mean = -29.74
Iteration 279: episode_reward_mean = -28.16
Iteration 280: episode_reward_mean = -28.72
Iteration 281: episode_reward_mean = -30.54
Iteration 282: episode_reward_mean = -31.67
Iteration 283: episode_reward_mean = -31.07
Iteration 284: episode_reward_mean = -30.24
Iteration 285: episode_reward_mean = -27.79
Iteration 286: episode_reward_mean = -27.99
Iteration 287: episode_reward_mean = -25.03
Iteration 288: episode_reward_mean = -26.95
Iteration 289: episode_reward_mean = -28.43
Iteration 290: episode_reward_mean = -30.79
Iteration 291: episode_reward_mean = -28.78
Iteration 292: episode_reward_mean = -27.32
Iteration 293: episode_reward_mean = -28.24
Iteration 294: episode_reward_mean = -30.15
Iteration 295: episode_reward_mean = -30.18
Iteration 296: episode_reward_mean = -27.40
Iteration 297: episode_reward_mean = -26.93
Iteration 298: episode_reward_mean = -27.06
Iteration 299: episode_reward_mean = -28.29
Iteration 300: episode_reward_mean = -28.52

[INFO] Checkpoint saved at: /Users/ali/ray_results/DQN_RandomizedAcrobot-v1_2025-07-08_16-32-4485kiah7t/checkpoint_000300

[INFO] Training Complete
Avg Training CPU Usage: 17.14%
Peak Memory Usage: 607.23 MB
Final Memory Change: 209.22 MB
Training Time: 646.38 seconds

Running trained DQN policy on RandomizedAcrobot-v1 and logging state evolution...

2025-07-08 16:43:31,403	WARNING util.py:66 -- Install gputil for GPU system monitoring.
2025-07-08 16:43:31,433	INFO trainable.py:766 -- Restored on 127.0.0.1 from checkpoint: /Users/ali/ray_results/DQN_RandomizedAcrobot-v1_2025-07-08_16-32-4485kiah7t/checkpoint_000300
2025-07-08 16:43:31,433	INFO trainable.py:775 -- Current state after restoring: {'_iteration': 300, '_timesteps_total': None, '_time_total': 642.7666921615601, '_episodes_total': 8700}
Inference log saved to acrobot_inference_DQN_log.csv

State plot saved to DQN-Acrobot-v1.png

